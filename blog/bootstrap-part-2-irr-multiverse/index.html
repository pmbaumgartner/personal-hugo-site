<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Bootstrapped Sampling for Annotation: A Multiverse of Madness? | Peter Baumgartner</title><meta name=keywords content><meta name=description content="Note: If you haven&rsquo;t read my prior post on the bootstrap and inter-rater reliability, this post probably won&rsquo;t make sense. Go read that first.
After I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.
What I failed to mention is that in the original example we were annotating data in a specific universe1."><meta name=author content><link rel=canonical href=https://www.peterbaumgartner.com/blog/bootstrap-part-2-irr-multiverse/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png><link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.105.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-72692144-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Bootstrapped Sampling for Annotation: A Multiverse of Madness?"><meta property="og:description" content="Note: If you haven&rsquo;t read my prior post on the bootstrap and inter-rater reliability, this post probably won&rsquo;t make sense. Go read that first.
After I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.
What I failed to mention is that in the original example we were annotating data in a specific universe1."><meta property="og:type" content="article"><meta property="og:url" content="https://www.peterbaumgartner.com/blog/bootstrap-part-2-irr-multiverse/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-11-10T04:00:00-04:00"><meta property="article:modified_time" content="2022-11-10T04:00:00-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Bootstrapped Sampling for Annotation: A Multiverse of Madness?"><meta name=twitter:description content="Note: If you haven&rsquo;t read my prior post on the bootstrap and inter-rater reliability, this post probably won&rsquo;t make sense. Go read that first.
After I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.
What I failed to mention is that in the original example we were annotating data in a specific universe1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"Bootstrapped Sampling for Annotation: A Multiverse of Madness?","item":"https://www.peterbaumgartner.com/blog/bootstrap-part-2-irr-multiverse/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bootstrapped Sampling for Annotation: A Multiverse of Madness?","name":"Bootstrapped Sampling for Annotation: A Multiverse of Madness?","description":"Note: If you haven\u0026rsquo;t read my prior post on the bootstrap and inter-rater reliability, this post probably won\u0026rsquo;t make sense. Go read that first.\nAfter I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.\nWhat I failed to mention is that in the original example we were annotating data in a specific universe1.","keywords":[],"articleBody":" Note: If you haven’t read my prior post on the bootstrap and inter-rater reliability, this post probably won’t make sense. Go read that first.\nAfter I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.\nWhat I failed to mention is that in the original example we were annotating data in a specific universe1. If you recall we were annotating examples incrementally by 100s until our 90% confidence interval was less than 0.2. But what determined the order those examples being annotated? In the example with the GoEmotions dataset of reddit comments I had sorted by comment creation date but failed to mention this in the blog post. It turns out that the order in which examples are annotated really matters, and in that specific universe things went pretty well.\nLet’s take a step back and enter two alternate universes where we had annotations for the the “disapproval” label (all examples in this post are using annotators 4 \u0026 61 on the “disapproval” label). In these two alternative universes I have two different random sampling processes that pull in new examples to be annotated. When I perform the bootstrap at 100 examples for both of these universes, here are the results:\nThese are two very different universes! In one universe, my kappa is looking quite good, with large variation. In the other, it’s looking awful - and the confidence interval is quite narrow!\nSo what’s the deal here? Well, we have to enter the multiverse to find out.\nEntering the Multiverse To evaluate the variability and dependence upon the annotation order I created 100 universes (simulations) wherein each universe a new random sample of all annotations was generated. Using this new sample, we re-run the same experiment in the original blog post, iterating by 100 samples through each of those annotations and identifying the iteration where we had enough examples that the 90% CI width was less than 0.2.\nWith this we can evaluate what the width of the confidence interval would be for each of these multiverses and get a sense of their variability. Here’s a plot of the width of the confidence intervals, for each sample size, for 100 multiverses:\nWe can see the variability at n=100 is quite large! The confidence interval widths range almost the full range of positive values.\nThis highlights a limitation of our approach, which is that it’s highly sensitive to the initial sample of overlapping annotations. But the limitations don’t stop there! Recall that we were using a width of the CI as our stopping criteria, and we decided that when we achieved a width of \u003c 0.2, we would stop. In our original analysis, this meant we stopped at n=700 annotations. But we can see from the chart above that even there we got lucky! Most of the CI widths at n=700 were greater than 0.2! We don’t see most of the widths getting below 0.2 until n=900, and a majority of them until n=1000!\nIn fact, here’s a chart of the first sample size (what we’ve been referring to as iteration) where a CI width under 0.2 is achieved for each of the 100 universe simulations:\nNow if we look at all of these universes that met the stopping criteria, we should check whether the confidence interval in each of those universes contains the “true” value of kappa - 0.334. When we do that, we discover that 94 of them do and 6 of them don’t2. That’s close to what we would expect since our confidence interval was 90%! Great job, statistics!\nFor more practical matters, let’s view the properties of those multiverses where the CIs do not contain the true value when we meet our stopping criteria. Here’s all of these universes, in a single plot to see their distributions.\nThere are two general classes of failure modes here. In the first case we have universes where at 100 samples we met our stopping criteria. In all of these cases, our estimate of κ is less than 0 - we have no agreement. In the other case, we stopped at a reasonable sample size but the samples don’t converge around our population value - they’re displaced slightly off of the population value.\nWhat to do? The main problem is that we don’t know what universe we’re in when we’re actually solving this problem. I think the best course of action here is to be aware of these failure modes and question what your results are showing us.\nIn the first case, when we’ve only got 100 annotations and we’re getting low agreement, the solution is to annotate more. Unless we have designed an extremely challenging novel task, it’s very unlikely that we have no (or negative) agreement between two annotators. It’s also important to consider the base rate of your task - if we’re annotating something that’s quite rare, then it doesn’t take many examples to get a scenario where it looks like there’s no agreement on the positive examples. In each of the instances here, if we were to annotate 100 more examples, for a total of 200 annotations, the CI width bounces back to a more realistic value (~0.35). Basically: 100 annotations will probably never be enough, unless we have a very well defined task with a balanced labels.\nIn the second case, when we have annotated a fair amount of examples and the 90% CI simply fails to capture the true value, lets consider the counterfactual this presents us. There were two types of scenarios here: one where our estimates were around 0.22, and another where they were around 0.44. In either case the kappa values are low. In the 0.22 case, that would tell us we need to reframe our annotation task because of low agreement. In the 0.44 case we need to consider whether that’s a reasonable rate of agreement for your task. Absent some other context, I’d say that’s still not quite good enough and we should redefine our task. Remember how the kappa values were correlated with F1 scores? We’d be looking at a 0.6-0.7 F1 with that level of value. Of course, we can never know this while we’re doing the task and this relationship might not generalize to all problems, but values of κ \u003c 0.6 generally indicate poor agreement.\nAnother option is to explore the source of the underlying disagreement in depth. Maybe we’ve selected two annotators that have very different conceptions of “disagreement” and are interpreting the annotation guidelines differently. In this case, we could add a third annotator and use a different metric to measure agreement. Another possibility is that the examples we’ve selected are genuinely difficult to annotate given our guidelines. We should review both the examples where there was agreement to understand their common features and explicate that in our annotation guidelines. We should also review those examples with disagreement and determine what features of those examples might have led to different annotations. Finally, remember that we went through the process of annotating data and we’re not just going to throw this away - we should use something like Prodigy’s review recipe to start building our gold dataset for evaluation.\nFinally, we could always change the stopping criteria to be more strict - we could increase the confidence interval to 95% or reduce the width to something like 0.15. However, the set of failure modes with low agreement might still occur here: because of the annotations available in those universes, we’d still likely see very tight CIs at 100 samples.\nWrap Up Now that this topic spans two blog posts, let’s summarize the problem and what we’ve learned.\nSummary: For machine learning projects, we want to know how much data we need to annotate to get an accurate model. However, we don’t know a lot of aspects about our problem—for example the base rate or difficulty of the annotation task—that would allow us to make a decision about how much data we need to annotate. Using inter-rater reliability (IRR) or inner-annotator agreement (IAA) metrics calculated from two or more annotators performing the annotation task can give us some information about our problem that will give us a better understanding of how much data we need to annotate. These metrics, when combined with bootstrap sampling, can be used to define a stopping criteria for an annotation project, allowing us to annotate just enough data to give us confidence in our metrics. A limitation of this methodology is that estimation is sensitive to initial conditions, specifically the order that examples are annotated. Because of this, we should be cautious in interpreting unlikely outcomes like no agreement and investigate disagreements in-depth. And of course, when in doubt: annotate more data.\nA commonly overlooked mistake. ↩︎\nThis will change depending on the random seed used. ↩︎\n","wordCount":"1496","inLanguage":"en","datePublished":"2022-11-10T04:00:00-04:00","dateModified":"2022-11-10T04:00:00-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/bootstrap-part-2-irr-multiverse/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.peterbaumgartner.com/ title=Home><span>Home</span></a></li><li><a href=https://www.peterbaumgartner.com/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks><span>Notebooks</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Bootstrapped Sampling for Annotation: A Multiverse of Madness?</h1><div class=post-meta><span title='2022-11-10 04:00:00 -0400 -0400'>November 10, 2022</span></div></header><div class=post-content><meta name=twitter:card content="summary"><meta name=twitter:site content="@pmbaumgartner"><meta name=twitter:creator content="@pmbaumgartner"><meta name=twitter:title content="Bootstrapped Sampling for Annotation: A Multiverse of Madness?"><meta name=twitter:description content="Giving Dr. Strange some company."><meta name=twitter:image content="https://i.ibb.co/8bbhQYq/multiverse-mountains.png"><blockquote><p><em>Note</em>: If you haven&rsquo;t read my <a href=/blog/how-much-data-bootstrap-irr>prior post</a> on the bootstrap and inter-rater reliability, this post probably won&rsquo;t make sense. Go read that first.</p></blockquote><p>After I had began discussing my last post with a few people, they had begun to replicate my analysis. Their replications raised an important issue I hope to address here: estimation is dependent on the initial sample of annotations that we have.</p><p>What I failed to mention is that in the original example we were annotating data in a specific <em>universe</em><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. If you recall we were annotating examples incrementally by 100s until our 90% confidence interval was less than 0.2. But what determined the order those examples being annotated? In the example with the GoEmotions dataset of reddit comments I had sorted by comment creation date but failed to mention this in the blog post. It turns out that the order in which examples are annotated really matters, and in that specific universe things went pretty well.</p><p>Let&rsquo;s take a step back and enter two alternate universes where we had annotations for the the &ldquo;disapproval&rdquo; label (all examples in this post are using annotators 4 & 61 on the &ldquo;disapproval&rdquo; label). In these two alternative universes I have two different random sampling processes that pull in new examples to be annotated. When I perform the bootstrap at 100 examples for both of these universes, here are the results:</p><p><img loading=lazy src=best-worst-universe.svg alt="Best and worst universe"></p><p>These are two <em>very</em> different universes! In one universe, my kappa is looking quite good, with large variation. In the other, it&rsquo;s looking awful - and the confidence interval is quite narrow!</p><p>So what&rsquo;s the deal here? Well, we have to enter the multiverse to find out.</p><h2 id=entering-the-multiverse>Entering the Multiverse<a hidden class=anchor aria-hidden=true href=#entering-the-multiverse>#</a></h2><p>To evaluate the variability and dependence upon the annotation order I created 100 universes (simulations) wherein each universe a new random sample of all annotations was generated. Using this new sample, we re-run the same experiment in the original blog post, iterating by 100 samples through each of those annotations and identifying the iteration where we had enough examples that the 90% CI width was less than 0.2.</p><p>With this we can evaluate what the width of the confidence interval would be for each of these multiverses and get a sense of their variability. Here&rsquo;s a plot of the width of the confidence intervals, for each sample size, for 100 multiverses:</p><p><img loading=lazy src=ci_widths_by_iter.svg alt="CI Widths by Iteration"></p><p>We can see the variability at n=100 is quite large! The confidence interval widths range almost the full range of positive values.</p><p>This highlights a limitation of our approach, which is that it&rsquo;s highly sensitive to the initial sample of overlapping annotations. But the limitations don&rsquo;t stop there! Recall that we were using a width of the CI as our stopping criteria, and we decided that when we achieved a width of &lt; 0.2, we would stop. In our original analysis, this meant we stopped at n=700 annotations. But we can see from the chart above that even there we got lucky! Most of the CI widths at n=700 were greater than 0.2! We don&rsquo;t see <em>most</em> of the widths getting below 0.2 until n=900, and a majority of them until n=1000!</p><p>In fact, here&rsquo;s a chart of the first sample size (what we&rsquo;ve been referring to as iteration) where a CI width under 0.2 is achieved for each of the 100 universe simulations:</p><p><img loading=lazy src=sample_size_stopping_criteria.svg alt="Sample Size Stopping Criteria"></p><p>Now if we look at all of these universes that met the stopping criteria, we should check whether the confidence interval in each of those universes contains the &ldquo;true&rdquo; value of kappa - <code>0.334</code>. When we do that, we discover that 94 of them do and 6 of them don&rsquo;t<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. That&rsquo;s close to what we would expect since our confidence interval was 90%! Great job, statistics!</p><p>For more practical matters, let&rsquo;s view the properties of those multiverses where the CIs do not contain the true value when we meet our stopping criteria. Here&rsquo;s all of these universes, in a single plot to see their distributions.</p><p><img loading=lazy src=ci_without_kappa.svg alt="CIs without population kappa"></p><p>There are two general classes of failure modes here. In the first case we have universes where at 100 samples we met our stopping criteria. In all of these cases, our estimate of <code>κ</code> is less than 0 - we have no agreement. In the other case, we stopped at a reasonable sample size but the samples don&rsquo;t converge around our population value - they&rsquo;re displaced slightly off of the population value.</p><h2 id=what-to-do>What to do?<a hidden class=anchor aria-hidden=true href=#what-to-do>#</a></h2><p>The main problem is that we don&rsquo;t know what universe we&rsquo;re in when we&rsquo;re actually solving this problem. I think the best course of action here is to be aware of these failure modes and question what your results are showing us.</p><p>In the first case, when we&rsquo;ve only got 100 annotations and we&rsquo;re getting low agreement, the solution is to annotate more. Unless we have designed an extremely challenging novel task, it&rsquo;s very unlikely that we have no (or negative) agreement between two annotators. It&rsquo;s also important to consider the base rate of your task - if we&rsquo;re annotating something that&rsquo;s quite rare, then it doesn&rsquo;t take many examples to get a scenario where it looks like there&rsquo;s no agreement on the positive examples. In each of the instances here, if we were to annotate 100 more examples, for a total of 200 annotations, the CI width bounces back to a more realistic value (~0.35). Basically: 100 annotations will probably never be enough, unless we have a very well defined task with a balanced labels.</p><p>In the second case, when we have annotated a fair amount of examples and the 90% CI simply fails to capture the true value, lets consider the counterfactual this presents us. There were two types of scenarios here: one where our estimates were around 0.22, and another where they were around 0.44. In either case the kappa values are low. In the 0.22 case, that would tell us we need to reframe our annotation task because of low agreement. In the 0.44 case we need to consider whether that&rsquo;s a reasonable rate of agreement for your task. Absent some other context, I&rsquo;d say that&rsquo;s still not quite good enough and we should redefine our task. Remember how the kappa values were correlated with F1 scores? We&rsquo;d be looking at a 0.6-0.7 F1 with that level of value. Of course, we can never know this while we&rsquo;re doing the task and this relationship might not generalize to all problems, but values of <code>κ &lt; 0.6</code> generally indicate <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/>poor agreement</a>.</p><p>Another option is to explore the source of the underlying disagreement in depth. Maybe we&rsquo;ve selected two annotators that have very different conceptions of &ldquo;disagreement&rdquo; and are interpreting the annotation guidelines differently. In this case, we could add a third annotator and use a <a href=https://en.wikipedia.org/wiki/Fleiss%27_kappa>different metric</a> to measure agreement. Another possibility is that the examples we&rsquo;ve selected are genuinely difficult to annotate given our guidelines. We should review both the examples where there was agreement to understand their common features and explicate that in our annotation guidelines. We should also review those examples with disagreement and determine what features of those examples might have led to different annotations. Finally, remember that we went through the process of annotating data and we&rsquo;re not just going to throw this away - we should use something like <a href=https://prodi.gy/docs/recipes/#review>Prodigy&rsquo;s review recipe</a> to start building our gold dataset for evaluation.</p><p>Finally, we could always change the stopping criteria to be more strict - we could increase the confidence interval to 95% or reduce the width to something like 0.15. However, the set of failure modes with low agreement might still occur here: because of the annotations available in those universes, we&rsquo;d still likely see very tight CIs at 100 samples.</p><h2 id=wrap-up>Wrap Up<a hidden class=anchor aria-hidden=true href=#wrap-up>#</a></h2><p>Now that this topic spans two blog posts, let&rsquo;s summarize the problem and what we&rsquo;ve learned.</p><p><strong>Summary:</strong> For machine learning projects, we want to know how much data we need to annotate to get an accurate model. However, we don&rsquo;t know a lot of aspects about our problem—for example the base rate or difficulty of the annotation task—that would allow us to make a decision about how much data we need to annotate. Using inter-rater reliability (IRR) or inner-annotator agreement (IAA) metrics calculated from two or more annotators performing the annotation task can give us some information about our problem that will give us a better understanding of how much data we need to annotate. These metrics, when combined with bootstrap sampling, can be used to define a stopping criteria for an annotation project, allowing us to annotate <em>just enough</em> data to give us confidence in our metrics. A limitation of this methodology is that estimation is sensitive to initial conditions, specifically the order that examples are annotated. Because of this, we should be cautious in interpreting unlikely outcomes like no agreement and investigate disagreements in-depth. And of course, when in doubt: annotate more data.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p> A commonly overlooked mistake.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p> This will change depending on the random seed used.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body><script data-goatcounter=https://peterbaumgartner.goatcounter.com/count async src=//gc.zgo.at/count.js></script></html>