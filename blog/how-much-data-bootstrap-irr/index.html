<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>"How much data do I need to label?" - The Bootstrapped Inter-Rater Reliability Answer | Peter Baumgartner</title><meta name=keywords content><meta name=description content="One of the most frequent questions that arises when doing applied ML projects is &ldquo;How much data to I need to label?&rdquo; When I get asked this question, I usually ask a few questions in return: what&rsquo;s the base rate of the outcome that you&rsquo;re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated1, or do you need to figure out some preprocessing to get an annotation-ready dataset?"><meta name=author content><link rel=canonical href=https://www.peterbaumgartner.com/blog/how-much-data-bootstrap-irr/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png><link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.109.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-72692144-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="&#34;How much data do I need to label?&#34; - The Bootstrapped Inter-Rater Reliability Answer"><meta property="og:description" content="One of the most frequent questions that arises when doing applied ML projects is &ldquo;How much data to I need to label?&rdquo; When I get asked this question, I usually ask a few questions in return: what&rsquo;s the base rate of the outcome that you&rsquo;re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated1, or do you need to figure out some preprocessing to get an annotation-ready dataset?"><meta property="og:type" content="article"><meta property="og:url" content="https://www.peterbaumgartner.com/blog/how-much-data-bootstrap-irr/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-10-28T11:39:27-04:00"><meta property="article:modified_time" content="2022-10-28T11:39:27-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="&#34;How much data do I need to label?&#34; - The Bootstrapped Inter-Rater Reliability Answer"><meta name=twitter:description content="One of the most frequent questions that arises when doing applied ML projects is &ldquo;How much data to I need to label?&rdquo; When I get asked this question, I usually ask a few questions in return: what&rsquo;s the base rate of the outcome that you&rsquo;re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated1, or do you need to figure out some preprocessing to get an annotation-ready dataset?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"\"How much data do I need to label?\" - The Bootstrapped Inter-Rater Reliability Answer","item":"https://www.peterbaumgartner.com/blog/how-much-data-bootstrap-irr/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"\"How much data do I need to label?\" - The Bootstrapped Inter-Rater Reliability Answer","name":"\u0022How much data do I need to label?\u0022 - The Bootstrapped Inter-Rater Reliability Answer","description":"One of the most frequent questions that arises when doing applied ML projects is \u0026ldquo;How much data to I need to label?\u0026rdquo; When I get asked this question, I usually ask a few questions in return: what\u0026rsquo;s the base rate of the outcome that you\u0026rsquo;re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated1, or do you need to figure out some preprocessing to get an annotation-ready dataset?","keywords":[],"articleBody":" One of the most frequent questions that arises when doing applied ML projects is “How much data to I need to label?” When I get asked this question, I usually ask a few questions in return: what’s the base rate of the outcome that you’re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated1, or do you need to figure out some preprocessing to get an annotation-ready dataset?\nThere’s a problem with all of these questions though: often times we don’t know the answer to these questions when starting a project, we only know them in hindsight.\nI was thinking about this issue the other day during a discussion of the data required for calculating inter-rater reliability (IRR) measures. I think IRR is one of the most useful and least discussed ideas in machine learning. Here’s the idea: if we’re performing some labeling task, an IRR measure gives us an idea of often how two or more people agree on a label. There’s a huge advantage to knowing this: if two reasonable humans don’t agree how things should be labeled, a machine learning model is going to have a hard time making reliable predictions on future data. Additionally, without multiple annotators, we might be biasing a model towards decisions of a single annotator - or at least a portion of the training data is biased from a single annotator from a pool. That’s going to be bad news when, in-production, a user inputs a piece of data similar to one a single annotator labeled, possibly incorrectly, and gets an unhelpful prediction.\nHopefully it’s clear IRR is useful after having labeled data, but how can it help us know how much data we would need to label? Enter the bootstrap!\nBootstrap sampling, or bootstrapping, is an intuitive idea proposed by Brad Efron in a 1979 paper. Rather than doing traditional statistical estimation, with bootstrap sampling we sample with replacement from the data that we have, calculate some measure, save that measure’s value, and repeat the process some number of times. The end result of this is a distribution of the measure of interest. With this distribution we can do helpful things like calculate the mean, median, or confidence intervals.\nNow let’s merge the two ideas of IRR and bootstrapping together. Let’s say we have a large pool of unlabeled data we’re looking to label. We don’t have any prior information about our task like the base rate or the difficulty of the task. What we’re going to do is start the annotation task by having two annotators label every example, then after n examples are double labeled, estimate some IRR measure with the bootstrap. Since we have a distribution of IRR values, we can now set a helpful stopping criteria: we can stop annotating when a confidence interval is narrower than some specified width. This means we don’t have to know what the “true” (population) IRR would be if we labeled all the data, only that we want to be fairly confident about knowing approximately what it is given some number of examples less than the full dataset.\nAn applied example Labeled datasets with multiple annotators are rare (Vincent has a short list), so we’ll use the GoEmotions dataset. For simplicity, we’ll only focus on two of the labels (“approval”, “disapproval”) from two annotators (4, 61) who had the most overlapping annotations (n=2,752). We’ll use Cohen’s Kappa (κ) as our IRR measure.\nWe’ll pretend we don’t have any labeled data and do a hypothetical simulation of how this would work in practice. What we’re going to do is pretend we’re starting from scratch with this dataset and we have a bunch of unlabeled reddit comments, and we’re going to have both annotators annotate all examples, and every 100 annotated examples we’re going to review the bootstrapped estimates of κ.\nOur threshold for ceasing dual annotation and reviewing our task will be when the 90% confidence interval has a width of less than 0.2. That’s a bit arbitrary, but when interpreting κ typically a new interpreted level of agreement occurs every 0.2 interval.\nAnnotating “approval” For examples labeled “approval”, the true (population) κ is 0.197 for Annotator 1 \u0026 2. This is the value we’d get if both annotators labeled all examples - a piece of information we have in hindsight, but not one we would have in our simulation. Annotator 1 had 496 positive labels, Annotator 2 had 322 positive labels, and they both agreed on 127 positive examples.\nLet’s pretend they’ve labeled the first 100 examples and calculate the bootstrap for κ. We’ll collect 500 samples of κ by sampling with replacement from our 100 annotated examples and calculating the value.\nFrom this chart we can see a few things: our sample range from no or random agreement (\u003c0) to fair agreement (\u003e0.5). It should be clear that we have very little confidence in what the true value of κ would be. The 90% confidence interval ranges from -0.030 to 0.383 - a width of about 0.4. We can conclude that at 100 labeled examples, we should continue labeling.\nLet’s pretend we continued this exercise, evaluating every 100 examples. Here’s what that would look like up to 1000 examples:\nWe can see our estimate of κ getting narrower and narrower. If we were to look at the distributional statistics for each iteration, we would find that we could stop annotating with overlap at 400 examples, when the confidence interval is 0.003 to 0.194 - a width of 0.191, which is less than 0.2.\nOne other thing to notice here is that the median value of κ at n=400 is 0.12, and our confidence interval doesn’t contain the true value of κ. This is a potential downside of this methodology2. We’re subject to the order in which we’re annotating examples and the samples that can be drawn from those examples. If we look back at the chart that continues this process, we’ll see the distributions start to center around the true value as add more examples. However, if we were actually doing this process in reality, we’d stop annotating here and never get to that correction with more samples.\nAnnotating “disapproval” We’ll repeat this exercise but now with the “disapproval” label. For this label, the true κ is 0.334 - slightly higher agreement than for the “approval” label. Annotator 1 has 251 positive labels, Annotator 2 has 132 positive labels, and they overlap for 72 positive labels.\nLet’s look again at what the bootstrapped distribution of κ would look like if we only labeled 100 with overlap.\nHere we see the values range almost the full scale of positive agreement values. The center appears to be a bit higher (median=0.660), but with this much spread it’s hard to say anything conclusive (except that we need to keep annotating).\nHere’s what the distributions look like as we continue the experiment every 100 annotations.\nFor this label the variability of the distributions is much wider across all iterations. Following our stopping rule would mean we would have annotated 700 examples, when the 90% CI is 0.269-0.464, with a width of 0.195.\nSummary \u0026 Implications Double annotating 400 and 700 examples in a binary classification task seems like a lot, right? But think of the costs of proceeding without an analysis like this: we end up wasting a lot of time and resources on generating a dataset on an under-specified task and generating lots of label errors anyways. The original dataset contains 58,011 individual comments and 211,225 labeled examples across the multiple annotators. By comparison, 700 is nothing - even with double labeling it’s not even 1% of the total number of labeled examples.\nOne thing we haven’t yet done is use our estimates of κ either. They have a use beyond understanding how much agreement our task has. They reported3 κ in the appendix of the original GoEmotions paper. If we plot their values of kappa against the F1 scores for each label, we might notice something interesting.\nThe kappa values are very strongly correlated (φ=0.904) with the F1 scores from the model. Because they’re that strongly correlated, in this instance, we actually get a fairly reliable proxy for how the model is going to perform. If we were actually starting an annotation task from scratch, we likely won’t even have a test set yet, so this is even more useful to make modeling, annotation, and problem framing decisions4!\nIn summary: how much data you need to label depends on characteristics inherent to your task and data, and you often don’t know these characteristics until you’ve completed the project. With the bootstrap and IRR measures, you can incrementally gain an understanding of these characteristics and make decisions accordingly as you annotate data.\nI’m going to use “annotation” and “labeling” interchangeably throughout this post. ↩︎\nOne possible way to alleviate this limitation is to adopt an idea like patience when using early stopping for training neural networks. Rather than stopping at the first instance of our criteria, we should wait until the 2nd or 3rd time in a row we meet the criteria to be sure it’s not just a fluke of this sample. ↩︎\nTheir calculation of kappa in the paper is so wrong, I’d consider it a proxy for the true agreement. However, it’s probably “close enough” to the real agreement for our use. ↩︎\nKeep in mind that if we wanted to do something like this, the aforementioned limitations around the sample ordering and sample size still apply. I’d probably up the CI to 95% or decrease the width of our threshold to something like 0.1. ↩︎\n","wordCount":"1619","inLanguage":"en","datePublished":"2022-10-28T11:39:27-04:00","dateModified":"2022-10-28T11:39:27-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/how-much-data-bootstrap-irr/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.peterbaumgartner.com/ title=Home><span>Home</span></a></li><li><a href=https://www.peterbaumgartner.com/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks><span>Notebooks</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>"How much data do I need to label?" - The Bootstrapped Inter-Rater Reliability Answer</h1><div class=post-meta><span title='2022-10-28 11:39:27 -0400 -0400'>October 28, 2022</span></div></header><div class=post-content><meta name=twitter:card content="summary"><meta name=twitter:site content="@pmbaumgartner"><meta name=twitter:creator content="@pmbaumgartner"><meta name=twitter:title content="&#34;How much data do I need to label?&#34;"><meta name=twitter:description content="Let's do the bootstrap and find out!"><meta name=twitter:image content="https://i.ibb.co/0rNMYXV/facesweird.png"><p>One of the most frequent questions that arises when doing applied ML projects is &ldquo;How much data to I need to label?&rdquo; When I get asked this question, I usually ask a few questions in return: what&rsquo;s the base rate of the outcome that you&rsquo;re labeling? Are you experimenting or building a production-ready system? How ambiguous or well-defined is your annotation task? Is all of your data ready to be annotated<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, or do you need to figure out some preprocessing to get an annotation-ready dataset?</p><p>There&rsquo;s a problem with all of these questions though: <strong>often times we don&rsquo;t know the answer to these questions when starting a project, we only know them in hindsight.</strong></p><p>I was thinking about this issue the other day during a discussion of the data required for calculating <a href=https://en.wikipedia.org/wiki/Inter-rater_reliability>inter-rater reliability</a> (IRR) measures. I think IRR is one of the most useful and least discussed ideas in machine learning. Here&rsquo;s the idea: if we&rsquo;re performing some labeling task, an IRR measure gives us an idea of often how two or more people agree on a label. There&rsquo;s a huge advantage to knowing this: if two reasonable humans don&rsquo;t agree how things should be labeled, a machine learning model is going to have a hard time making reliable predictions on future data. Additionally, without multiple annotators, we might be biasing a model towards decisions of a single annotator - or at least a portion of the training data is biased from a single annotator from a pool. That&rsquo;s going to be bad news when, in-production, a user inputs a piece of data similar to one a single annotator labeled, possibly incorrectly, and gets an unhelpful prediction.</p><p>Hopefully it&rsquo;s clear IRR is useful after having labeled data, but how can it help us know how much data we would need to label? Enter <a href=https://en.wikipedia.org/wiki/Bootstrapping_(statistics)>the bootstrap</a>!</p><p>Bootstrap sampling, or bootstrapping, is an intuitive idea proposed by Brad Efron in a <a href=https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full>1979 paper</a>. Rather than doing traditional statistical estimation, with bootstrap sampling we sample with replacement from the data that we have, calculate some measure, save that measure&rsquo;s value, and repeat the process some number of times. The end result of this is a distribution of the measure of interest. With this distribution we can do helpful things like calculate the mean, median, or confidence intervals.</p><p>Now let&rsquo;s merge the two ideas of IRR and bootstrapping together. Let&rsquo;s say we have a large pool of unlabeled data we&rsquo;re looking to label. We don&rsquo;t have any prior information about our task like the base rate or the difficulty of the task. What we&rsquo;re going to do is start the annotation task by having two annotators label every example, then after <code>n</code> examples are double labeled, estimate some IRR measure with the bootstrap. Since we have a distribution of IRR values, we can now set a helpful stopping criteria: we can stop annotating when a confidence interval is narrower than some specified width. This means we don&rsquo;t have to know what the &ldquo;true&rdquo; (population) IRR would be if we labeled all the data, only that we want to be fairly confident about knowing approximately what it is given some number of examples less than the full dataset.</p><h2 id=an-applied-example>An applied example<a hidden class=anchor aria-hidden=true href=#an-applied-example>#</a></h2><p>Labeled datasets with multiple annotators are rare (<a href=https://koaning.io/til/2022-10-07-annotation-datasets/>Vincent has a short list</a>), so we&rsquo;ll use the <a href=https://arxiv.org/abs/2005.00547>GoEmotions</a> dataset. For simplicity, we&rsquo;ll only focus on two of the labels (&ldquo;approval&rdquo;, &ldquo;disapproval&rdquo;) from two annotators (4, 61) who had the most overlapping annotations (<code>n=2,752</code>). We&rsquo;ll use <a href=https://en.wikipedia.org/wiki/Cohen%27s_kappa>Cohen&rsquo;s Kappa</a> (<code>κ</code>) as our IRR measure.</p><p>We&rsquo;ll pretend we don&rsquo;t have any labeled data and do a hypothetical simulation of how this would work in practice. What we&rsquo;re going to do is pretend we&rsquo;re starting from scratch with this dataset and we have a bunch of unlabeled reddit comments, and we&rsquo;re going to have both annotators annotate all examples, and every 100 annotated examples we&rsquo;re going to review the bootstrapped estimates of <code>κ</code>.</p><p>Our threshold for ceasing dual annotation and reviewing our task will be when the 90% confidence interval has a width of less than <code>0.2</code>. That&rsquo;s a bit arbitrary, but when interpreting <code>κ</code> typically a new interpreted level of agreement occurs <a href=https://www.statology.org/cohens-kappa-statistic/>every <code>0.2</code> interval</a>.</p><h3 id=annotating-approval>Annotating &ldquo;approval&rdquo;<a hidden class=anchor aria-hidden=true href=#annotating-approval>#</a></h3><p>For examples labeled &ldquo;approval&rdquo;, the true (population) <code>κ</code> is <code>0.197</code> for Annotator 1 & 2. This is the value we&rsquo;d get if both annotators labeled all examples - a piece of information we have in hindsight, but not one we would have in our simulation. Annotator 1 had 496 positive labels, Annotator 2 had 322 positive labels, and they both agreed on 127 positive examples.</p><p>Let&rsquo;s pretend they&rsquo;ve labeled the first 100 examples and calculate the bootstrap for <code>κ</code>. We&rsquo;ll collect 500 samples of <code>κ</code> by sampling with replacement from our 100 annotated examples and calculating the value.</p><p><img loading=lazy src=approval100.svg alt="Distribution of κ at 100 examples"></p><p>From this chart we can see a few things: our sample range from no or random agreement (&lt;0) to fair agreement (>0.5). It should be clear that we have very little confidence in what the true value of <code>κ</code> would be. The 90% confidence interval ranges from -0.030 to 0.383 - a width of about 0.4. We can conclude that at 100 labeled examples, we should continue labeling.</p><p>Let&rsquo;s pretend we continued this exercise, evaluating every 100 examples. Here&rsquo;s what that would look like up to 1000 examples:</p><p><img loading=lazy src=approval-all.svg alt="Distribution of κ after n labeled examples"></p><p>We can see our estimate of <code>κ</code> getting narrower and narrower. If we were to look at the distributional statistics for each iteration, we would find that we could stop annotating with overlap at 400 examples, when the confidence interval is 0.003 to 0.194 - a width of 0.191, which is less than 0.2.</p><p>One other thing to notice here is that the median value of <code>κ</code> at <code>n=400</code> is 0.12, and our confidence interval doesn&rsquo;t contain the true value of <code>κ</code>. This is a potential downside of this methodology<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. We&rsquo;re subject to the order in which we&rsquo;re annotating examples and the samples that can be drawn from those examples. If we look back at the chart that continues this process, we&rsquo;ll see the distributions start to center around the true value as add more examples. However, if we were actually doing this process in reality, we&rsquo;d stop annotating here and never get to that correction with more samples.</p><h3 id=annotating-disapproval>Annotating &ldquo;disapproval&rdquo;<a hidden class=anchor aria-hidden=true href=#annotating-disapproval>#</a></h3><p>We&rsquo;ll repeat this exercise but now with the &ldquo;disapproval&rdquo; label. For this label, the true <code>κ</code> is <code>0.334</code> - slightly higher agreement than for the &ldquo;approval&rdquo; label. Annotator 1 has 251 positive labels, Annotator 2 has 132 positive labels, and they overlap for 72 positive labels.</p><p>Let&rsquo;s look again at what the bootstrapped distribution of <code>κ</code> would look like if we only labeled 100 with overlap.</p><p><img loading=lazy src=disapproval100.svg alt="Distribution of κ at 100 examples"></p><p>Here we see the values range almost the full scale of positive agreement values. The center appears to be a bit higher (median=0.660), but with this much spread it&rsquo;s hard to say anything conclusive (except that we need to keep annotating).</p><p>Here&rsquo;s what the distributions look like as we continue the experiment every 100 annotations.</p><p><img loading=lazy src=disapproval-all.svg alt="Distribution of κ after n labeled examples"></p><p>For this label the variability of the distributions is much wider across all iterations. Following our stopping rule would mean we would have annotated 700 examples, when the 90% CI is 0.269-0.464, with a width of 0.195.</p><h3 id=summary--implications>Summary & Implications<a hidden class=anchor aria-hidden=true href=#summary--implications>#</a></h3><p>Double annotating 400 and 700 examples in a binary classification task seems like a lot, right? But think of the costs of proceeding without an analysis like this: we end up wasting a lot of time and resources on generating a dataset on an under-specified task and generating lots of <a href=https://www.surgehq.ai/blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled>label errors</a> anyways. The original dataset contains 58,011 individual comments and 211,225 labeled examples across the multiple annotators. By comparison, 700 is nothing - even with double labeling it&rsquo;s not even 1% of the total number of labeled examples.</p><p>One thing we haven&rsquo;t yet done is use our estimates of <code>κ</code> either. They have a use beyond understanding how much agreement our task has. They reported<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> <code>κ</code> in the appendix of the original GoEmotions paper. If we plot their values of kappa against the F1 scores for each label, we might notice something interesting.</p><p><img loading=lazy src=kappaf1.svg alt="Kappa and F1 Scores from GoEmotions labels"></p><p>The kappa values are very strongly correlated (φ=0.904) with the F1 scores from the model. Because they&rsquo;re that strongly correlated, in this instance, we actually get a fairly reliable proxy for how the model is going to perform. If we were actually starting an annotation task from scratch, we likely won&rsquo;t even have a test set yet, so this is even more useful to make modeling, annotation, and problem framing decisions<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>!</p><p>In summary: how much data you need to label depends on characteristics inherent to your task and data, and you often don&rsquo;t know these characteristics until you&rsquo;ve completed the project. With the bootstrap and IRR measures, you can incrementally gain an understanding of these characteristics and make decisions accordingly as you annotate data.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>I&rsquo;m going to use &ldquo;annotation&rdquo; and &ldquo;labeling&rdquo; interchangeably throughout this post.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>One possible way to alleviate this limitation is to adopt an idea like <em>patience</em> when using <a href=https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/>early stopping</a> for training neural networks. Rather than stopping at the first instance of our criteria, we should wait until the 2nd or 3rd time in a row we meet the criteria to be sure it&rsquo;s not just a fluke of this sample.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Their calculation of kappa in the paper is so wrong, I&rsquo;d consider it a proxy for the true agreement. However, it&rsquo;s probably &ldquo;close enough&rdquo; to the real agreement for our use.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Keep in mind that if we wanted to do something like this, the aforementioned limitations around the sample ordering and sample size still apply. I&rsquo;d probably up the CI to 95% or decrease the width of our threshold to something like 0.1.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body><script data-goatcounter=https://peterbaumgartner.goatcounter.com/count async src=//gc.zgo.at/count.js></script></html>