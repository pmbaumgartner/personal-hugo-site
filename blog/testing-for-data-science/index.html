<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Ways I Use Testing as a Data Scientist | Peter Baumgartner</title>
<meta name=keywords content>
<meta name=description content="In my work, writing tests serves three purposes: making sure things work, documenting my understanding, preventing future errors. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked&ndash;I was staring right at the output in a notebook and visually inspecting that the output was correct.
After gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test requires some experience in knowing what can go wrong.">
<meta name=author content>
<link rel=canonical href=https://www.peterbaumgartner.com/blog/testing-for-data-science/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png>
<link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.91.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-72692144-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Ways I Use Testing as a Data Scientist">
<meta property="og:description" content="In my work, writing tests serves three purposes: making sure things work, documenting my understanding, preventing future errors. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked&ndash;I was staring right at the output in a notebook and visually inspecting that the output was correct.
After gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test requires some experience in knowing what can go wrong.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.peterbaumgartner.com/blog/testing-for-data-science/"><meta property="article:section" content="blog">
<meta property="article:published_time" content="2021-12-22T19:48:32-05:00">
<meta property="article:modified_time" content="2021-12-22T19:48:32-05:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Ways I Use Testing as a Data Scientist">
<meta name=twitter:description content="In my work, writing tests serves three purposes: making sure things work, documenting my understanding, preventing future errors. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked&ndash;I was staring right at the output in a notebook and visually inspecting that the output was correct.
After gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test requires some experience in knowing what can go wrong.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"Ways I Use Testing as a Data Scientist","item":"https://www.peterbaumgartner.com/blog/testing-for-data-science/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ways I Use Testing as a Data Scientist","name":"Ways I Use Testing as a Data Scientist","description":"In my work, writing tests serves three purposes: making sure things work, documenting my understanding, preventing future errors. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked\u0026ndash;I was staring right at the output in a notebook and visually inspecting that the output was correct.\nAfter gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test requires some experience in knowing what can go wrong.","keywords":[],"articleBody":"In my work, writing tests serves three purposes: making sure things work, documenting my understanding, preventing future errors. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked–I was staring right at the output in a notebook and visually inspecting that the output was correct.\nAfter gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test requires some experience in knowing what can go wrong. It requires making mistakes and encountering issues so we know what needs to be tested when we encounter a similar problem. One complicating factor on top of this is that beginner mistakes are often syntactical (i.e. “how do I get this code to actually run”) and not conceptual or domain-specific. Telling a beginner to write a test every time they encounter a syntax error to make sure they don’t do that again is not valuable.\nAs a data scientist, I wear many different hats, which also made learning about testing difficult. There’s plenty of material on testing from a software development perspective, but if I’m doing an analysis and not developing software, I found many of those concepts difficult to translate and apply in my work.\nIn that spirit, I thought I would write a blog post on the many ways I use testing in my work, in hopes that other data scientists will find it helpful when they’re trying to figure out what to test and how to test in the code they write.\nTesting Analysis \u0026 Processing Code: assert When doing a one-time or ad hoc analysis, the assert statement in python is my go-to tool. Typically in an analysis, I use assert statements on as many intermediate calculations or processes as I can.\nOne example is merging two datasets by some common id. In this example, assume we have some knowledge that there should be no IDs that are in one data set and not the other. In this case, we can write a quick assert like the following:\nids1 = set(df1[\"ID\"].unique()) ids2 = set(df2[\"ID\"].unique()) assert len(ids1.symmetric_difference(ids2)) == 0, \"One Dataset Contains Exclusive IDs\" In this test, we create a set out of the unique identifiers (assuming they’re in a DataFrame), and check that the symmetric difference between those two sets is 0.\nPro Tip: It’s possible to add an expression that runs when the assert fails by including it after the assertion expression, separated by a comma. We can use this fact to make a failed assert even more helpful when debugging, e.g.:\nassert len(ids1.symmetric_difference(ids2)) == 0, f\"DF1 not DF2: {ids1 - ids2}- DF2 not DF1: {ids2 - ids1}\" Another thing to check is basic calculations and arithmetic. Recently I was analyzing a survey that had some logic determining who should be asked what questions, where I needed to check that the total number of responses to a certain question added up to the number of “Yes” responses to a prior question. My data was in crosstabs generated by pandas, so my quick check looked something like this:\nExample Crosstab Count Percent Yes 30 .3 No 50 .5 Missing 20 .2 prior_question_yes = prior_question_crosstab.loc[\"Yes\", \"Count\"] subsequent_question_total = subquestion_question_crosstab[\"Count\"].sum() assert prior_question_yes == subsequent_question_total, \"Subsequent Question Total Not Matching\" You might be thinking “this is too obvious of a thing to even test”, but it saved me much heartache as I looped over the pairs of parent/child questions and realized I had two typos in my question mapping and my code was referencing the wrong data frames in those cases. This is an important aspect of tests: while they may seem less valuable when running them on a single aspect of the data, they are very helpful as we write code that scales to touching multiple aspects of the data.\nWriting More Tests While I’ve mostly migrated away from notebooks, for some projects they still make sense. One practice I’ve started is that whenever I visually investigate some aspect of my data by writing some disposable code in a notebook, I convert that validation into an assert statement.\nAs a beginner, one might have an idea for what to test but struggle to find the right tools to write tests. One place to look for help is the documentation and test suite of the libraries that are being used. For example, to check that two arrays of floats are close to each other, with the caveat they might not be exactly the same, there’s np.isclose. We can see use of np.isclose in the numpy test suite. If we’re using pandas, they also have a helpful testing module in case we need to do things like check if two DataFrames are equal.\nIdentifying New Tests \u0026 Testing Code that Operates on Data: Hypothesis If we have a function that operates on data and have a hard time figuring out what to test, hypothesis is a great library that can help.\nOne way I’ve used this is analyzing code that operated on a Likert-style question from a survey. These are the type of survey questions that go from “strongly disagree” to “strongly agree”, and each value is associated with a number (e.g. 1 to 5).\nI wrote a function that returned a bunch of information about a column of these values, similar to the following:\ndef summarize_likert(series: pd.Series): n = len(series) n_completed = series.notnull().sum() mean = series.mean() empty = series.isnull().sum() pct_empty = empty / n small = n_completed  7 summary = dict( n=n, n_completed=n_completed, mean=mean, empty=empty, pct_empty=pct_empty, small=small, ) return summary Pretty simple, right? Now I’ll show how to use hypothesis to run some tests with this type of function. There is a bit of setup to create a hypothesis strategy that mimics our data, but we only have to do it once and it’s reusable.\nfrom hypothesis import assume, given, strategies as st from hypothesis.extra.pandas import range_indexes, series # DEFINE STRATEGY @st.composite def plus_nan(draw, strat): return draw(st.one_of(st.just(np.nan), strat)) index_strategy = range_indexes(min_size=0, max_size=500) likert_data = st.integers(min_value=1, max_value=5) likert_data_with_nan = plus_nan(likert_data) likert_series = series(elements=likert_data, index=index_strategy) likert_series_with_nan = series(elements=likert_data_with_nan, index=index_strategy) # CREATE TEST @given(likert_series_with_nan) def test_likert_na(series): summary = summarize_likert(series) # Likert are 1-5, so mean is in that interval assert 1  summary[\"mean\"]  5 # EXECUTE TEST if __name__ == \"__main__\": # actually run test # can use pytest as well test_likert_na() If we run this simple test, we’ll get the following error:\nRuntimeWarning: invalid value encountered in long_scalars pct_empty = empty / n Falsifying example: test_likert_na( series=Series([], dtype: float64), ) This is hypothesis telling us we had an error at runtime with the code and showing us the example it generated that gave that error. It turns out we have behavior that doesn’t work when passed an empty Series. At first I thought this wasn’t worth considering, but in my use case I was often automatically filtering data based on other columns, so it is realistic at some point I might pass an empty series and want to know about it. At this point, we’d write some behavior to handle the case where we are passed an empty series. A simple fix would be to add the following at the top of our function and add an assumption to the test that the series is not empty.\ndef summarize_likert(series: pd.Series) if series.empty: raise ValueError(\"Series Empty\") ... @given(likert_series_with_nan) def test_likert_na(series): assume(not series.empty) summary = summarize_likert(series) # Likert are 1-5, so mean is in that interval assert 1  summary[\"mean\"]  5 So we’ve fixed that issue, let’s run the test again.\nFalsifying example: test_likert_na( series=0 NaN dtype: float64, ) Traceback (most recent call last): File \"likert.py\", line 50, in  test_likert_na() File \"likert.py\", line 41, in test_likert_na @settings(verbosity=Verbosity.verbose) File \"/Users/peter/opt/miniconda3/envs/blog-test/lib/python3.8/site-packages/hypothesis/core.py\", line 1190, in wrapped_test raise the_error_hypothesis_found File \"likert.py\", line 46, in test_likert_na assert 1 A similar issue: we haven’t defined what happens when the series contains all NaN values. If we look up further in the traceback, we actually see that hypothesis is smart enough that it found a more complex example that didn’t work (A longer series of NaN), and then reduced it down to the simpler case with only one NaN.\nTests on the Data: pandera \u0026 Great Expectations It’s a good idea to write tests on the data itself. To do this, we’ll need to do a bit of exploratory work to understand the qualities of the data, then translate that into tests. Testing data is extremely helpful if we will be repeatedly receiving new data with the same structure: tests are a quick way to make sure there are no issues with new data.\nFor lightweight use cases, I like pandera. With pandera, I do two high level activities with a dataset: explicitly define a schema for the data and add in any supplemental info I have about the data structure.\nFor the first definition of the schema, all I do is define the column names and types. We can use the infer_schema method to get started with this with an existing DataFrame. From here, we need to manually validate column types and make corrections. For example, an object type column might be better as a categorical variable, and we’ll want to check that any timestamp columns are correctly inferred. Even something as simple as this has saved me from a costly error when a new dataset I received was missing a column I was expecting from the first version of the dataset.\nFrom there, I create v2 of the schema, which adds Checks to the columns. Checks are information we gain after exploring the data – for example, whether a column should always be positive, whether the column name should be formatted a certain way, or whether a column should only contain certain values (e.g. a bool represented as a 0/1 int).\nIf we’re expecting to repetedly read in new data, I would recommend exploring Great Expectations. The killer feature of Great Expectations is that it will generate a template of tests for the data based on a sample set of data we give it, like pandera’s infer_schema on steroids. Again, this is only a starting point for adding in future tests (or expectations), but can be really helpful in generating basic things to test.\nGreat Expectations is a little more involved to setup, so I think the investment is worth it if we know we’ll be repeatedly reading in new data with the same structure. There are also some stellar additional features like the data docs, which has been helpful for me communicating any data quality issues with a larger team.\nFinally, even if we are not expecting new versions of the data, writing tests about the data is still a good idea. It’s great documentation for ourselves when we come back to this project or when others join our team. Additionally, it gives us something to communicate to others to help in validating data assumptions.\nWriting Code for Other People: Pytest The final way I use tests is if I’m writing software for other people. Two recent libraries I’ve written, SetFit and EmBuddy, are examples of this. In this case, I take a more traditional software testing approach and use pytest to create and execute tests.\nMy testing approach is close to Test-Driven Development where I typically write a test first. I’m not rigorous with this, but I do use this process to sketch out the API I want to create so I can get a better picture of how people will interact with my code.\nHere’s one example from EmBuddy that tests the save and load functionality. I know I want save and load to be as simple as possible from an API perspective – just provide a path and save the object there. Given that, I wrote a test with the API for this how I imagined it before I wrote the actual functionality. It looks like this:\ndef test_persist_str(tmp_path, embuddy_sm): path = str(tmp_path / \"test_embeddings.emb\") emb1 = embuddy_sm emb1.embed([\"this is a sentence\"]) emb1.save(path) emb2 = EmBuddy.load(path) assert np.array_equal(emb1.embedding_cache, emb2.embedding_cache) There are some advanced pytest features going on here to explain. First is that this test definition includes two arguments: those are actually fixtures, which are objects commonly used across tests. In this case I’m using a fixture that comes with pytest, tmp_path, to create a temporary path to save and another fixture, and embuddy_sm, which is a pre-created instance of an Embuddy object. The utility of these fixtures is that I don’t have to rewrite the code to create a temporary folder or Embuddy object every time I want to use those in a test – especially important if that test isn’t testing the functionality related to creating those things.\nThe test structure itself follows a common pattern called Arrange-Act-Assert. Until I learned about this, I really struggled trying to figure out how to write tests. As a bonus, this pattern fits nicely into how tests are written withpytest. In the above example, I arrange by including the fixtures, act by saving the model, and assert that the results from the initial model and persisted model are the same.\nEven if we’re not sure what to assert, writing a test that executes the code is still valuable. Sometimes I write code that raises an exception when I run it because I made a mistake or forgot to implement something – so the value of a test was literally just running that code again and knowing that I had made a mistake that I needed to fix. When fixing a mistake, it’s a good idea to also convert that fix into a test as well.\nHere’s an example of that from Embuddy. After attempting to ask for the nearest neighbors before I had built the index to do so, I realized I needed to add in an instructive error for other users who might make that same mistake. In this case, I created a custom exception and then tested that exception was raised when that same mistake was made, like so:\ndef test_no_index_exception(embuddy_sm): with pytest.raises(IndexNotBuiltError): embuddy_sm.nearest_neighbors(\"Some text\") Wrap-Up There’s plenty of things to test in doing data-science work, but it’s not always clear what to test or how you should test it. In my experience, I’m usually testing one of the following things:\n The results of some analysis process (using assert) Code that operates on data (using hypothesis) Aspects of the data (using pandera or Great Expectations) Code for others (using pytest)  If you’re a data scientist and test other things or have other tools, reach out and let me know.\n","wordCount":"2439","inLanguage":"en","datePublished":"2021-12-22T19:48:32-05:00","dateModified":"2021-12-22T19:48:32-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/testing-for-data-science/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://www.peterbaumgartner.com/ title=Home>
<span>Home</span>
</a>
</li>
<li>
<a href=https://www.peterbaumgartner.com/blog/ title=Blog>
<span>Blog</span>
</a>
</li>
<li>
<a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks>
<span>Notebooks</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Ways I Use Testing as a Data Scientist
</h1>
<div class=post-meta><span title="2021-12-22 19:48:32 -0500 -0500">December 22, 2021</span>
</div>
</header>
<div class=post-content><meta name=twitter:card content="summary">
<meta name=twitter:site content="@pmbaumgartner">
<meta name=twitter:creator content="@pmbaumgartner">
<meta name=twitter:title content="Ways I Use Testing as a Data Scientist">
<meta name=twitter:description content="How I think about testing code & testing data">
<meta name=twitter:image content="https://i.ibb.co/LYz2DyQ/1537215809678.jpg">
<p>In my work, writing tests serves three purposes: <em>making sure things work</em>, <em>documenting my understanding</em>, <em>preventing future errors</em>. When I was starting out with testing, I had a hard time understanding what I should be writing tests for. As a beginner, I just assumed my code worked&ndash;I was staring right at the output in a notebook and visually inspecting that the output was correct.</p>
<p>After gaining some experience writing tests, I realized one of my problems initially was the fact that knowing what to test <em>requires some experience in knowing what can go wrong</em>. It requires making mistakes and encountering issues so we know what needs to be tested when we encounter a similar problem. One complicating factor on top of this is that beginner mistakes are often <em>syntactical</em> (i.e. &ldquo;how do I get this code to actually run&rdquo;) and not <em>conceptual</em> or <em>domain-specific</em>. Telling a beginner to write a test every time they encounter a syntax error to make sure they don&rsquo;t do that again is not valuable.</p>
<p>As a data scientist, I wear many different hats, which also made learning about testing difficult. There&rsquo;s plenty of material on testing from a software development perspective, but if I&rsquo;m doing an analysis and not developing software, I found many of those concepts difficult to translate and apply in my work.</p>
<p>In that spirit, I thought I would write a blog post on the many ways I use testing in my work, in hopes that other data scientists will find it helpful when they&rsquo;re trying to figure out <em>what</em> to test and <em>how</em> to test in the code they write.</p>
<h2 id=testing-analysis--processing-code-assert>Testing Analysis & Processing Code: <code>assert</code><a hidden class=anchor aria-hidden=true href=#testing-analysis--processing-code-assert>#</a></h2>
<p>When doing a one-time or ad hoc analysis, the <code>assert</code> statement in python is my go-to tool. Typically in an analysis, I use assert statements on as many intermediate calculations or processes as I can.</p>
<p>One example is merging two datasets by some common id. In this example, assume we have some knowledge that there should be no IDs that are in one data set and not the other. In this case, we can write a quick assert like the following:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>ids1 <span style=color:#f92672>=</span> set(df1[<span style=color:#e6db74>&#34;ID&#34;</span>]<span style=color:#f92672>.</span>unique())
ids2 <span style=color:#f92672>=</span> set(df2[<span style=color:#e6db74>&#34;ID&#34;</span>]<span style=color:#f92672>.</span>unique())

<span style=color:#66d9ef>assert</span> len(ids1<span style=color:#f92672>.</span>symmetric_difference(ids2)) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;One Dataset Contains Exclusive IDs&#34;</span>
</code></pre></div><p>In this test, we create a set out of the unique identifiers (assuming they&rsquo;re in a <code>DataFrame</code>), and check that the <a href=https://docs.python.org/3.8/library/stdtypes.html#frozenset.symmetric_difference>symmetric difference</a> between those two sets is 0.</p>
<p><strong>Pro Tip:</strong> It&rsquo;s possible to add an expression that runs when the assert fails by including it after the assertion expression, separated by a comma. We can use this fact to make a failed assert even more helpful when debugging, e.g.:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>assert</span> len(ids1<span style=color:#f92672>.</span>symmetric_difference(ids2)) <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;DF1 not DF2: </span><span style=color:#e6db74>{</span>ids1 <span style=color:#f92672>-</span> ids2<span style=color:#e6db74>}</span><span style=color:#e6db74> - DF2 not DF1: </span><span style=color:#e6db74>{</span>ids2 <span style=color:#f92672>-</span> ids1<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</code></pre></div><p>Another thing to check is basic calculations and arithmetic. Recently I was analyzing a survey that had some logic determining who should be asked what questions, where I needed to check that the total number of responses to a certain question added up to the number of &ldquo;Yes&rdquo; responses to a prior question. My data was in crosstabs generated by <code>pandas</code>, so my quick check looked something like this:</p>
<pre tabindex=0><code>Example Crosstab 
         Count    Percent
Yes         30         .3
No          50         .5
Missing     20         .2
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>prior_question_yes <span style=color:#f92672>=</span> prior_question_crosstab<span style=color:#f92672>.</span>loc[<span style=color:#e6db74>&#34;Yes&#34;</span>, <span style=color:#e6db74>&#34;Count&#34;</span>]

subsequent_question_total <span style=color:#f92672>=</span> subquestion_question_crosstab[<span style=color:#e6db74>&#34;Count&#34;</span>]<span style=color:#f92672>.</span>sum()

<span style=color:#66d9ef>assert</span> prior_question_yes <span style=color:#f92672>==</span> subsequent_question_total, <span style=color:#e6db74>&#34;Subsequent Question Total Not Matching&#34;</span>
</code></pre></div><p>You might be thinking &ldquo;this is too obvious of a thing to even test&rdquo;, but it saved me much heartache as I looped over the pairs of parent/child questions and realized I had two typos in my question mapping and my code was referencing the wrong data frames in those cases. This is an important aspect of tests: while they may seem less valuable when running them on a single aspect of the data, they are very helpful as we write code that scales to touching multiple aspects of the data.</p>
<p><strong>Writing More Tests</strong>
While I&rsquo;ve mostly migrated away from notebooks, for some projects they still make sense. One practice I&rsquo;ve started is that whenever I <em>visually</em> investigate some aspect of my data by writing some disposable code in a notebook, I convert that validation into an assert statement.</p>
<p>As a beginner, one might have an idea for <em>what</em> to test but struggle to find the right tools to write tests. One place to look for help is the documentation and test suite of the libraries that are being used. For example, to check that two arrays of floats are close to each other, with the caveat they might not be <em>exactly</em> the same, there&rsquo;s <a href=https://numpy.org/doc/stable/reference/generated/numpy.isclose.html><code>np.isclose</code></a>. We can see use of <code>np.isclose</code> in the <a href=https://github.com/numpy/numpy/blob/main/numpy/core/tests/test_numeric.py#L2471>numpy test suite</a>. If we&rsquo;re using <code>pandas</code>, they also have a helpful testing module in case we need to do things like <a href=https://pandas.pydata.org/docs/reference/api/pandas.testing.assert_frame_equal.html>check if two DataFrames are equal</a>.</p>
<h2 id=identifying-new-tests--testing-code-that-operates-on-data-hypothesis>Identifying New Tests & Testing Code that Operates on Data: <code>Hypothesis</code><a hidden class=anchor aria-hidden=true href=#identifying-new-tests--testing-code-that-operates-on-data-hypothesis>#</a></h2>
<p>If we have a function that operates on data and have a hard time figuring out what to test, <a href=https://hypothesis.readthedocs.io/en/latest/>hypothesis</a> is a great library that can help.</p>
<p>One way I&rsquo;ve used this is analyzing code that operated on a <a href=https://en.wikipedia.org/wiki/Likert_scale>Likert</a>-style question from a survey. These are the type of survey questions that go from &ldquo;strongly disagree&rdquo; to &ldquo;strongly agree&rdquo;, and each value is associated with a number (e.g. 1 to 5).</p>
<p>I wrote a function that returned a bunch of information about a column of these values, similar to the following:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>summarize_likert</span>(series: pd<span style=color:#f92672>.</span>Series):
    n <span style=color:#f92672>=</span> len(series)
    n_completed <span style=color:#f92672>=</span> series<span style=color:#f92672>.</span>notnull()<span style=color:#f92672>.</span>sum()
    mean <span style=color:#f92672>=</span> series<span style=color:#f92672>.</span>mean()
    empty <span style=color:#f92672>=</span> series<span style=color:#f92672>.</span>isnull()<span style=color:#f92672>.</span>sum()
    pct_empty <span style=color:#f92672>=</span> empty <span style=color:#f92672>/</span> n
    small <span style=color:#f92672>=</span> n_completed <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>7</span>
    summary <span style=color:#f92672>=</span> dict(
        n<span style=color:#f92672>=</span>n,
        n_completed<span style=color:#f92672>=</span>n_completed,
        mean<span style=color:#f92672>=</span>mean,
        empty<span style=color:#f92672>=</span>empty,
        pct_empty<span style=color:#f92672>=</span>pct_empty,
        small<span style=color:#f92672>=</span>small,
    )
    <span style=color:#66d9ef>return</span> summary
</code></pre></div><p>Pretty simple, right? Now I&rsquo;ll show how to use <code>hypothesis</code> to run some tests with this type of function. There is a bit of setup to create a hypothesis strategy that mimics our data, but we only have to do it once and it&rsquo;s reusable.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
<span style=color:#f92672>from</span> hypothesis <span style=color:#f92672>import</span> assume, given, strategies <span style=color:#66d9ef>as</span> st
<span style=color:#f92672>from</span> hypothesis.extra.pandas <span style=color:#f92672>import</span> range_indexes, series

<span style=color:#75715e># DEFINE STRATEGY</span>
<span style=color:#a6e22e>@st</span><span style=color:#f92672>.</span>composite
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plus_nan</span>(draw, strat):
    <span style=color:#66d9ef>return</span> draw(st<span style=color:#f92672>.</span>one_of(st<span style=color:#f92672>.</span>just(np<span style=color:#f92672>.</span>nan), strat))


index_strategy <span style=color:#f92672>=</span> range_indexes(min_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, max_size<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>)
likert_data <span style=color:#f92672>=</span> st<span style=color:#f92672>.</span>integers(min_value<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, max_value<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>)
likert_data_with_nan <span style=color:#f92672>=</span> plus_nan(likert_data)

likert_series <span style=color:#f92672>=</span> series(elements<span style=color:#f92672>=</span>likert_data, index<span style=color:#f92672>=</span>index_strategy)
likert_series_with_nan <span style=color:#f92672>=</span> series(elements<span style=color:#f92672>=</span>likert_data_with_nan, index<span style=color:#f92672>=</span>index_strategy)


<span style=color:#75715e># CREATE TEST</span>
<span style=color:#a6e22e>@given</span>(likert_series_with_nan)
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_likert_na</span>(series):
    summary <span style=color:#f92672>=</span> summarize_likert(series)
    <span style=color:#75715e># Likert are 1-5, so mean is in that interval</span>
    <span style=color:#66d9ef>assert</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>&lt;=</span> summary[<span style=color:#e6db74>&#34;mean&#34;</span>] <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>5</span>

<span style=color:#75715e># EXECUTE TEST</span>
<span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
	<span style=color:#75715e># actually run test</span>
	<span style=color:#75715e># can use pytest as well</span>
    test_likert_na()
</code></pre></div><p>If we run this simple test, we&rsquo;ll get the following error:</p>
<pre tabindex=0><code>RuntimeWarning: invalid value encountered in long_scalars
  pct_empty = empty / n
Falsifying example: test_likert_na(
    series=Series([], dtype: float64),
)
</code></pre><p>This is <code>hypothesis</code> telling us we had an error at runtime with the code and showing us the example it generated that gave that error. It turns out we have behavior that doesn&rsquo;t work when passed an empty <code>Series</code>. At first I thought this wasn&rsquo;t worth considering, but in my use case I was often automatically filtering data based on other columns, so it is realistic at some point I might pass an empty series and want to know about it. At this point, we&rsquo;d write some behavior to handle the case where we are passed an empty series. A simple fix would be to add the following at the top of our function and add an assumption to the test that the series is not empty.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>summarize_likert</span>(series: pd<span style=color:#f92672>.</span>Series)
	<span style=color:#66d9ef>if</span> series<span style=color:#f92672>.</span>empty:
   		<span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#34;Series Empty&#34;</span>)
	<span style=color:#f92672>...</span>

	
<span style=color:#a6e22e>@given</span>(likert_series_with_nan)
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_likert_na</span>(series):
	assume(<span style=color:#f92672>not</span> series<span style=color:#f92672>.</span>empty)
    summary <span style=color:#f92672>=</span> summarize_likert(series)
    <span style=color:#75715e># Likert are 1-5, so mean is in that interval</span>
    <span style=color:#66d9ef>assert</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>&lt;=</span> summary[<span style=color:#e6db74>&#34;mean&#34;</span>] <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>5</span>
</code></pre></div><p>So we&rsquo;ve fixed that issue, let&rsquo;s run the test again.</p>
<pre tabindex=0><code>Falsifying example: test_likert_na(
    series=0   NaN
    dtype: float64,
)
Traceback (most recent call last):
  File &quot;likert.py&quot;, line 50, in &lt;module&gt;
    test_likert_na()
  File &quot;likert.py&quot;, line 41, in test_likert_na
    @settings(verbosity=Verbosity.verbose)
  File &quot;/Users/peter/opt/miniconda3/envs/blog-test/lib/python3.8/site-packages/hypothesis/core.py&quot;, line 1190, in wrapped_test
    raise the_error_hypothesis_found
  File &quot;likert.py&quot;, line 46, in test_likert_na
    assert 1 &lt;= summary[&quot;mean&quot;] &lt;= 5
AssertionError
</code></pre><p>A similar issue: we haven&rsquo;t defined what happens when the series contains all <code>NaN</code> values. If we look up further in the traceback, we actually see that <code>hypothesis</code> is smart enough that it found a more complex example that didn&rsquo;t work (A longer series of <code>NaN</code>), and then reduced it down to the simpler case with only one <code>NaN</code>.</p>
<h2 id=tests-on-the-data-pandera--great-expectations>Tests on the Data: <code>pandera</code> & Great Expectations<a hidden class=anchor aria-hidden=true href=#tests-on-the-data-pandera--great-expectations>#</a></h2>
<p>It&rsquo;s a good idea to write tests on the data itself. To do this, we&rsquo;ll need to do a bit of exploratory work to understand the qualities of the data, then translate that into tests. Testing data is extremely helpful if we will be repeatedly receiving new data with the same structure: tests are a quick way to make sure there are no issues with new data.</p>
<p>For lightweight use cases, I like <a href=https://pandera.readthedocs.io/en/stable/><code>pandera</code></a>. With <code>pandera</code>, I do two high level activities with a dataset: explicitly define a schema for the data and add in any supplemental info I have about the data structure.</p>
<p>For the first definition of the schema, all I do is define the column names and types. We can use the <a href=https://pandera.readthedocs.io/en/stable/schema_inference.html><code>infer_schema</code></a> method to get started with this with an existing DataFrame. From here, we need to manually validate column types and make corrections. For example, an <code>object</code> type column might be better as a categorical variable, and we&rsquo;ll want to check that any timestamp columns are correctly inferred. Even something as simple as this has saved me from a costly error when a new dataset I received was missing a column I was expecting from the first version of the dataset.</p>
<p>From there, I create v2 of the schema, which adds <a href=https://pandera.readthedocs.io/en/stable/checks.html>Checks</a> to the columns. Checks are information we gain after exploring the data &ndash; for example, whether a column should always be positive, whether the column name should be formatted a certain way, or whether a column should only contain certain values (e.g. a <code>bool</code> represented as a <code>0/1 int</code>).</p>
<p>If we&rsquo;re expecting to repetedly read in new data, I would recommend exploring <a href=https://docs.greatexpectations.io/docs/>Great Expectations</a>. The killer feature of Great Expectations is that it will generate a template of tests for the data based on a sample set of data we give it, like <code>pandera</code>&rsquo;s <code>infer_schema</code> on steroids. Again, this is only a starting point for adding in future tests (or <em>expectations</em>), but can be really helpful in generating basic things to test.</p>
<p>Great Expectations is a little more involved to setup, so I think the investment is worth it if we know we&rsquo;ll be repeatedly reading in new data with the same structure. There are also some stellar additional features like the data docs, which has been helpful for me communicating any data quality issues with a larger team.</p>
<p>Finally, even if we are not expecting new versions of the data, writing tests about the data is still a good idea. It&rsquo;s great documentation for ourselves when we come back to this project or when others join our team. Additionally, it gives us something to communicate to others to help in validating data assumptions.</p>
<h2 id=writing-code-for-other-people-pytest>Writing Code for Other People: <code>Pytest</code><a hidden class=anchor aria-hidden=true href=#writing-code-for-other-people-pytest>#</a></h2>
<p>The final way I use tests is if I&rsquo;m writing software for other people. Two recent libraries I&rsquo;ve written, <a href=https://github.com/pmbaumgartner/setfit>SetFit</a> and <a href=https://github.com/pmbaumgartner/embuddy>EmBuddy</a>, are examples of this. In this case, I take a more traditional software testing approach and use <a href=https://docs.pytest.org/>pytest</a> to create and execute tests.</p>
<p>My testing approach is close to <a href=https://www.agilealliance.org/glossary/tdd/>Test-Driven Development</a> where I typically write a test first. I&rsquo;m not rigorous with this, but I do use this process to sketch out the API I want to create so I can get a better picture of how people will interact with my code.</p>
<p>Here&rsquo;s one example from EmBuddy that tests the save and load functionality. I know I want save and load to be as simple as possible from an API perspective &ndash; just provide a path and save the object there. Given that, I wrote a test with the API for this <em>how I imagined it</em> before I wrote the actual functionality. It looks like this:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_persist_str</span>(tmp_path, embuddy_sm):
    path <span style=color:#f92672>=</span> str(tmp_path <span style=color:#f92672>/</span> <span style=color:#e6db74>&#34;test_embeddings.emb&#34;</span>)
    emb1 <span style=color:#f92672>=</span> embuddy_sm
    emb1<span style=color:#f92672>.</span>embed([<span style=color:#e6db74>&#34;this is a sentence&#34;</span>])
    emb1<span style=color:#f92672>.</span>save(path)

    emb2 <span style=color:#f92672>=</span> EmBuddy<span style=color:#f92672>.</span>load(path)
    <span style=color:#66d9ef>assert</span> np<span style=color:#f92672>.</span>array_equal(emb1<span style=color:#f92672>.</span>embedding_cache, emb2<span style=color:#f92672>.</span>embedding_cache)
</code></pre></div><p>There are some advanced <code>pytest</code> features going on here to explain. First is that this test definition includes two arguments: those are actually fixtures, which are objects commonly used across tests. In this case I&rsquo;m using a fixture that comes with <code>pytest</code>, <code>tmp_path</code>, to create a temporary path to save and another fixture, and <code>embuddy_sm</code>, which is a pre-created instance of an Embuddy object. The utility of these fixtures is that I don&rsquo;t have to rewrite the code to create a temporary folder or Embuddy object every time I want to use those in a test &ndash; especially important if that test isn&rsquo;t testing the functionality related to creating those things.</p>
<p>The test structure itself follows a common pattern called <a href=https://automationpanda.com/2020/07/07/arrange-act-assert-a-pattern-for-writing-good-tests/>Arrange-Act-Assert</a>. Until I learned about this, I really struggled trying to figure out how to write tests. As a bonus, this pattern fits nicely into how tests are written with<code>pytest</code>. In the above example, I <em>arrange</em> by including the fixtures, <em>act</em> by saving the model, and <em>assert</em> that the results from the initial model and persisted model are the same.</p>
<p><strong>Even if we&rsquo;re not sure what to assert, writing a test that executes the code is still valuable.</strong> Sometimes I write code that raises an exception when I run it because I made a mistake or forgot to implement something &ndash; so the value of a test was literally just running that code again and knowing that I had made a mistake that I needed to fix. When fixing a mistake, it&rsquo;s a good idea to also convert that fix into a test as well.</p>
<p>Here&rsquo;s an example of that from Embuddy. After attempting to ask for the nearest neighbors before I had built the index to do so, I realized I needed to add in an instructive error for other users who might make that same mistake. In this case, I created a custom exception and then tested that exception was raised when that same mistake was made, like so:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_no_index_exception</span>(embuddy_sm):
    <span style=color:#66d9ef>with</span> pytest<span style=color:#f92672>.</span>raises(IndexNotBuiltError):
        embuddy_sm<span style=color:#f92672>.</span>nearest_neighbors(<span style=color:#e6db74>&#34;Some text&#34;</span>)
</code></pre></div><h2 id=wrap-up>Wrap-Up<a hidden class=anchor aria-hidden=true href=#wrap-up>#</a></h2>
<p>There&rsquo;s plenty of things to test in doing data-science work, but it&rsquo;s not always clear what to test or how you should test it. In my experience, I&rsquo;m usually testing one of the following things:</p>
<ol>
<li>The results of some analysis process (using <code>assert</code>)</li>
<li>Code that operates on data (using <code>hypothesis</code>)</li>
<li>Aspects of the data (using <code>pandera</code> or <code>Great Expectations</code>)</li>
<li>Code for others (using <code>pytest</code>)</li>
</ol>
<p>If you&rsquo;re a data scientist and test other things or have other tools, <a href=https://twitter.com/pmbaumgartner>reach out</a> and let me know.</p>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>