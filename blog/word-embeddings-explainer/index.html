<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Word Embeddings Explainer | Peter Baumgartner</title>
<meta name=keywords content>
<meta name=description content="What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?">
<meta name=author content>
<link rel=canonical href=https://www.peterbaumgartner.com/blog/word-embeddings-explainer/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png>
<link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-72692144-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Word Embeddings Explainer">
<meta property="og:description" content="What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.peterbaumgartner.com/blog/word-embeddings-explainer/"><meta property="article:section" content="blog">
<meta property="article:published_time" content="2018-04-30T15:22:07-04:00">
<meta property="article:modified_time" content="2018-04-30T15:22:07-04:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Word Embeddings Explainer">
<meta name=twitter:description content="What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"Word Embeddings Explainer","item":"https://www.peterbaumgartner.com/blog/word-embeddings-explainer/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Word Embeddings Explainer","name":"Word Embeddings Explainer","description":"What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.\nFor a visual example, here are simplified word embeddings for common 4- and 5-letter english words.\nI\u0026rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.\nWhat are they good for?","keywords":[],"articleBody":"What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.\nFor a visual example, here are simplified word embeddings for common 4- and 5-letter english words.\nI’ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.\nWhat are they good for? A word embedding model transforms words into numbers so that we can do interesting measurements with them. Below are some applications of word embeddings.\nWord Similarity You can ask a word who its neighbors are. With enough words, this similarity query can work like a synonym finder. In the 2D grid representation above, each immediate neighbor is equally distant from some central term, but with the full dimension word embeddings you can get a more nuanced similarity score for each word.\nSentence Similarity Sentences are made of words, so we can calculate a semantic distance between sentences using embeddings. Here’s 3 sample sentences, with words we have addresses for in bold.\n There’s a tall patch of grass. The root of the tree is in soil. Let’s chat about pizza and cake!  It’s clear to us, as humans, that the first two sentences share meaning and the 3rd sentence is different than them. A common way of calculating the similarity of two pieces of text is to count how many words overlap — however, none of the sentences above have any meaningful words in common (once we remove common stopwords like “the”). When measuring similarity in this way, all three of our example sentences are equally similar, since they all don’t share any words. With word embeddings, we can use semantic distance for this case.\nMeasuring semantic similarity with embeddings With word embeddings there are a few ways to measure the similarity of two sentences.\n Average Embeddings - Find the average location (centroid) of the words in both sentences. Then measure the distance between these two average locations. Word Movers Distance - Find the total cost of moving from all the words in one sentence to all the words in another sentence. Soft Cosine Similarity - Combine the magic of linear algebra and the location of your word embeddings to create a new space to measure similarity. (There’s a lot behind this one, but you should know that its fast and it works well.)  What’s the difference between this simplification and actual word embeddings? Word embeddings are usually more than 2 dimensions (commonly 50, 100, 200, and 300). For this example, I used the 300D vectors trained on Google News. That high dimensionality is helpful for solving large computational problems, and each dimension captures some unique semantic structure of the words given text the model was trained on.\nTo get to the 2D Map of Wordville, I did the following:\n I used this helpful list of common english words as a source. I looped through that list in order, picking out words that were 4- or 5-letters long (so their label would fit in the map) and were semantically similar (measured with a word embedding model) to some basic seed words (car, dog, food) until I had 625 words. Projected the vectors of these words down to two dimensions with a technique called Universal Manifold Approximation and Projection (UMAP). Used an optimization algorithm to map each point in 2D to a grid of 2D points, following this notebook.  How can I make my own word embeddings? The gensim library in python allows you to create your own word embeddings. It requires your input text to be tokenized.\nHere’s a basic example – we will create a model from 10000 copies of 4 tokenized sentences. (Note: You will not get good results with this input data.)\nimport gensim tokenized_sentences = [ ['I', 'love', 'my', 'cat', '.'], ['I', 'love', 'my', 'dog', '!'], ['I', 'love', 'my', 'cat', ',', 'sometimes', '.'], ['I', 'also', 'like', 'my', 'dog', '!'] ] many_tokenized_sentences = tokenized_sentences * 10000 # create the model model = gensim.models.Word2Vec(sentences=many_tokenized_sentences) # find similar words model.wv.most_similar('cat') One thing to note is that in comparison to other common NLP tasks, you will not want to remove stopwords or punctuation from the vocabulary, since they play an important role in providing context.\n","wordCount":"717","inLanguage":"en","datePublished":"2018-04-30T15:22:07-04:00","dateModified":"2018-04-30T15:22:07-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/word-embeddings-explainer/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://www.peterbaumgartner.com/ title=Home>
<span>Home</span>
</a>
</li>
<li>
<a href=https://www.peterbaumgartner.com/blog/ title=Blog>
<span>Blog</span>
</a>
</li>
<li>
<a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks>
<span>Notebooks</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Word Embeddings Explainer
</h1>
<div class=post-meta><span title="2018-04-30 15:22:07 -0400 -0400">April 30, 2018</span>
</div>
</header>
<div class=post-content><meta name=twitter:card content="summary">
<meta name=twitter:site content="@pmbaumgartner">
<meta name=twitter:creator content="@pmbaumgartner">
<meta name=twitter:title content="Word Embeddings Explainer">
<meta name=twitter:description content="What are word embeddings and what can we do with them?">
<meta name=twitter:image content="https://github.com/pmbaumgartner/personal-hugo-site/raw/master/static/images/wordville.png">
<h3 id=what-are-word-embeddings>What are word embeddings?<a hidden class=anchor aria-hidden=true href=#what-are-word-embeddings>#</a></h3>
<p>Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.</p>
<p>For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.</p>
<p><img loading=lazy src=/images/wordville.png alt=Wordville>
</p>
<p>I&rsquo;ve drawn 3 <em>neighborhoods</em> over this embedding to illustrate the semantic groupings.</p>
<h3 id=what-are-they-good-for>What are they good for?<a hidden class=anchor aria-hidden=true href=#what-are-they-good-for>#</a></h3>
<p>A word embedding model transforms words into numbers so that we can do interesting measurements with them. Below are some applications of word embeddings.</p>
<h4 id=word-similarity>Word Similarity<a hidden class=anchor aria-hidden=true href=#word-similarity>#</a></h4>
<p>You can ask a word who its neighbors are. With enough words, this similarity query can work like a synonym finder. In the 2D grid representation above, each immediate neighbor is equally distant from some central term, but with the full dimension word embeddings you can get a more nuanced similarity score for each word.</p>
<h4 id=sentence-similarity>Sentence Similarity<a hidden class=anchor aria-hidden=true href=#sentence-similarity>#</a></h4>
<p>Sentences are made of words, so we can calculate a semantic distance between sentences using embeddings. Here&rsquo;s 3 sample sentences, with words we have addresses for in bold.</p>
<ol>
<li>There&rsquo;s a <strong>tall</strong> <strong>patch</strong> of <strong>grass</strong>.</li>
<li>The <strong>root</strong> of the <strong>tree</strong> is in <strong>soil</strong>.</li>
<li>Let&rsquo;s <strong>chat</strong> about <strong>pizza</strong> and <strong>cake</strong>!</li>
</ol>
<p>It&rsquo;s clear to us, as humans, that the first two sentences share meaning and the 3rd sentence is different than them. A common way of calculating the similarity of two pieces of text is to count how many words overlap — however, none of the sentences above have any meaningful words in common (once we remove common <em><a href=https://en.wikipedia.org/wiki/Stop_words>stopwords</a></em> like &ldquo;the&rdquo;). When measuring similarity in this way, all three of our example sentences are equally similar, since they all don&rsquo;t share any words. With word embeddings, we can use semantic distance for this case.</p>
<h5 id=measuring-semantic-similarity-with-embeddings>Measuring semantic similarity with embeddings<a hidden class=anchor aria-hidden=true href=#measuring-semantic-similarity-with-embeddings>#</a></h5>
<p>With word embeddings there are a few ways to measure the similarity of two sentences.</p>
<ol>
<li><strong>Average Embeddings</strong> - Find the <em>average</em> location (centroid) of the words in both sentences. Then measure the distance between these two <em>average</em> locations.</li>
<li><strong>Word Movers Distance</strong> - Find the total cost of moving from all the words in one sentence to all the words in another sentence.</li>
<li><strong>Soft Cosine Similarity</strong> - Combine the magic of linear algebra and the location of your word embeddings to create a new space to measure similarity. (There&rsquo;s a lot behind this one, but you should know that its fast and it <a href=http://www.redalyc.org/html/615/61532067007/>works well</a>.)</li>
</ol>
<h3 id=whats-the-difference-between-this-simplification-and-actual-word-embeddings>What&rsquo;s the difference between this simplification and actual word embeddings?<a hidden class=anchor aria-hidden=true href=#whats-the-difference-between-this-simplification-and-actual-word-embeddings>#</a></h3>
<p>Word embeddings are usually more than 2 dimensions (commonly 50, 100, 200, and 300). For this example, I used the 300D vectors trained on Google News. That high dimensionality is helpful for solving large computational problems, and each dimension captures some unique semantic structure of the words given text the model was trained on.</p>
<p>To get to the 2D Map of Wordville, I did the following:</p>
<ol>
<li>I used <a href=https://norvig.com/ngrams/>this</a> helpful list of common english words as a source. I looped through that list in order, picking out words that were 4- or 5-letters long (so their label would fit in the map) and were semantically similar (measured with a word embedding model) to some basic seed words (car, dog, food) until I had 625 words.</li>
<li>Projected the vectors of these words down to two dimensions with a technique called <a href=https://github.com/lmcinnes/umap>Universal Manifold Approximation and Projection</a> (UMAP).</li>
<li>Used an optimization algorithm to map each point in 2D to a grid of 2D points, following <a href=https://github.com/kylemcdonald/CloudToGrid/blob/master/CloudToGrid.ipynb>this notebook</a>.</li>
</ol>
<h3 id=how-can-i-make-my-own-word-embeddings>How can I make my own word embeddings?<a hidden class=anchor aria-hidden=true href=#how-can-i-make-my-own-word-embeddings>#</a></h3>
<p>The <a href=https://github.com/RaRe-Technologies/gensim>gensim</a> library in python allows you to create your own word embeddings. It requires your input text to be tokenized.</p>
<p>Here&rsquo;s a basic example &ndash; we will create a model from 10000 copies of 4 tokenized sentences. (Note: You will not get good results with this input data.)</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> gensim 

tokenized_sentences <span style=color:#f92672>=</span> [
    [<span style=color:#e6db74>&#39;I&#39;</span>, <span style=color:#e6db74>&#39;love&#39;</span>, <span style=color:#e6db74>&#39;my&#39;</span>, <span style=color:#e6db74>&#39;cat&#39;</span>, <span style=color:#e6db74>&#39;.&#39;</span>],
    [<span style=color:#e6db74>&#39;I&#39;</span>, <span style=color:#e6db74>&#39;love&#39;</span>, <span style=color:#e6db74>&#39;my&#39;</span>, <span style=color:#e6db74>&#39;dog&#39;</span>, <span style=color:#e6db74>&#39;!&#39;</span>],
    [<span style=color:#e6db74>&#39;I&#39;</span>, <span style=color:#e6db74>&#39;love&#39;</span>, <span style=color:#e6db74>&#39;my&#39;</span>, <span style=color:#e6db74>&#39;cat&#39;</span>, <span style=color:#e6db74>&#39;,&#39;</span>, <span style=color:#e6db74>&#39;sometimes&#39;</span>, <span style=color:#e6db74>&#39;.&#39;</span>],
    [<span style=color:#e6db74>&#39;I&#39;</span>, <span style=color:#e6db74>&#39;also&#39;</span>, <span style=color:#e6db74>&#39;like&#39;</span>, <span style=color:#e6db74>&#39;my&#39;</span>, <span style=color:#e6db74>&#39;dog&#39;</span>, <span style=color:#e6db74>&#39;!&#39;</span>]
]

many_tokenized_sentences <span style=color:#f92672>=</span> tokenized_sentences <span style=color:#f92672>*</span> <span style=color:#ae81ff>10000</span>

<span style=color:#75715e># create the model</span>
model <span style=color:#f92672>=</span> gensim<span style=color:#f92672>.</span>models<span style=color:#f92672>.</span>Word2Vec(sentences<span style=color:#f92672>=</span>many_tokenized_sentences)

<span style=color:#75715e># find similar words</span>
model<span style=color:#f92672>.</span>wv<span style=color:#f92672>.</span>most_similar(<span style=color:#e6db74>&#39;cat&#39;</span>)

</code></pre></div><p>One thing to note is that in comparison to other common NLP tasks, you will not want to remove stopwords or punctuation from the vocabulary, since they play an important role in providing context.</p>
</div>
<footer class=post-footer>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>