<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Fine-Tuning GPT-2 Small for Generative Text | Peter Baumgartner</title><meta name=keywords content><meta name=description content="Why did the chicken cross the road? Because it had no legs.
These are the types of hilarious jokes the gpt-2 small model can generate for you.
After reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.
For this example, we&rsquo;ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes."><meta name=author content><link rel=canonical href=https://www.peterbaumgartner.com/blog/gpt2-jokes/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png><link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.109.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-72692144-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Fine-Tuning GPT-2 Small for Generative Text"><meta property="og:description" content="Why did the chicken cross the road? Because it had no legs.
These are the types of hilarious jokes the gpt-2 small model can generate for you.
After reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.
For this example, we&rsquo;ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes."><meta property="og:type" content="article"><meta property="og:url" content="https://www.peterbaumgartner.com/blog/gpt2-jokes/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2019-03-23T11:16:39-04:00"><meta property="article:modified_time" content="2019-03-23T11:16:39-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Fine-Tuning GPT-2 Small for Generative Text"><meta name=twitter:description content="Why did the chicken cross the road? Because it had no legs.
These are the types of hilarious jokes the gpt-2 small model can generate for you.
After reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.
For this example, we&rsquo;ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"Fine-Tuning GPT-2 Small for Generative Text","item":"https://www.peterbaumgartner.com/blog/gpt2-jokes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fine-Tuning GPT-2 Small for Generative Text","name":"Fine-Tuning GPT-2 Small for Generative Text","description":"Why did the chicken cross the road? Because it had no legs.\nThese are the types of hilarious jokes the gpt-2 small model can generate for you.\nAfter reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.\nFor this example, we\u0026rsquo;ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes.","keywords":[],"articleBody":" Why did the chicken cross the road? Because it had no legs.\nThese are the types of hilarious jokes the gpt-2 small model can generate for you.\nAfter reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.\nFor this example, we’ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes. You’ll need a computer with a GPU and nvidia-docker for this.\nGetting Started We’ll start by cloning the code to download and train the GPT-2 Small model. Fortunately, others have done the hard work of adding code to train on top of the gpt-2 small model that OpenAI released.\ngit clone https://github.com/nshepperd/gpt-2 cd gpt-2 sh download_model.sh 117M We’re going to use docker from here on out, just because it’s easier to manage the code and dependencies. The repository comes with a dockerfile, let’s build the image:\ndocker build --tag gpt-2 -f Dockerfile.gpu . Great! Let’s get to a shell using our image:\ndocker run --runtime=nvidia -it \\ -v $(pwd):/gpt-2 \\ -e NVIDIA_VISIBLE_DEVICES=0 \\ gpt-2 bash At this point, you can play with the base gpt-2 small model and generate some text. Let’s try it out with this prompt:\n“A pair of jumper cables walks into a bar”\n$ python src/interactive_conditional_samples.py --top_k 40 --temperature 0.9 [...some tensorflow logging] Model prompt \u003e\u003e\u003e A pair of jumper cables walks into a bar ======================================== SAMPLE 1 ======================================== and a few minutes later, they are all turned on. A man in jeans and a black dress, comes out of the bar and says he got a job and wanted to know if he could tell me what he is wearing, how much he is wearing and whether he's wearing something different. He's wearing a red skirt, a blue T-shirt and black heels, on a black tie, like a red cape with two red crosses, the same as on the white one. The original punchline was:\nThe bartender sighs and says; “I’ll serve you, but don’t start anything!”\nThat’s the level of sense we can expect out of this thing without fine tuning.\nFine-Tuning on a Specific Corpus We’re going to fine-tune it a set of jokes. What this is going to do is train the model to pick up both the structure of the joke (setup, punchline) as well as how language is used (both vocabulary and structure).\nWe’ll download a set of jokes from this repository. Note that there is some NSFW and racist, sexist, and plain unfunny content in some of these jokes, look out for this both in the training data and in our model output.\nThere are a few different structures for jokes in our dataset. For short jokes, the setup will be the title of the post, and the punchline will be the body. For longer jokes that have a bit of setup, typically the title of the post is also the first sentence of the joke. Fine-tuning the model will pick up on the structure and language of the jokes, so what we’ll do is separate the setup (post title) and punchline (post body) with a pipe (|). Additionally, gpt-2 uses a special token \u003c|endoftext|\u003e, to signify the end of a piece of text, so we’ll be formatting data with those structural constraints.\nFirst, let’s download the data:\ncurl -O https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json The following python script will get the data in the format we need:\nimport json from pathlib import Path jokes_raw = json.loads(Path(\"reddit_jokes.json\").read_text()) jokes_parsed = \"\u003c|endoftext|\u003e\".join(\"{0}|{1}\".format(j['title'], j['body']) for j in jokes_raw) Path(\"input-text.txt\").write_text(jokes_parsed) (side note: pathlib is amazing, you should be using it if you aren’t already)\nNow we must encode our text in the input format for gpt-2. This is something that normally would happen during training anyway, but we can speed it up by preprocessing.\nThe requirements.txt needs to be updated with the tqdm dependency, so you will need to install it in the container before running the encoding script.\npip install tqdm PYTHONPATH=src ./encode.py input-text.txt input-text.txt.npz And with that, let’s get training! This script will train until you Ctrl-C out of it. The flags that we’re passing will print out a sample as well as checkpoint the model every 250 epochs.\nPYTHONPATH=src ./train.py --dataset input-text.txt.npz --sample_every=250 --save_every=250 Choosing the right amount of training for these types of things is difficult. Personally, I enjoy the surrealism, absurdity, and nonsense of jokes from models with fewer training epochs.\nTraining Examples Below I’ll post some jokes from the sample output that gets generated every 250 epochs:\n250:\nWhat do you call it when the baby is on your chin?|Lion's Pie Did you hear about the guy who lost a job for eating his lunch?|He was just going to eat it. I tried to take my dog to the bathroom in the hospital...|My dog was a dachshund. What did the priest say after an elephant died?|\"What is it for?\" 500 epochs:\nI bought my car last week, and it wasn't working really well.|I was going to give it a better start, but the engine just wasn't good enough. 750:\nTwo dogs are walking through the woods|When they meet up the first dog yells \"Hey is this dog our friend?\". The second dog thinks it's funny and says \"Is this dog that he walks around alone\". And the two dogs quickly stop walking. \"I don't think so, your mom always walks by his yard.\" Why did the chicken cross the road?|To get a job at the hen. What do you call a man with dyslexia?|A dyslexic. A man walks into a bar and asks for a drink|Bartender looks at him and says, \"This is not a drinking game.\" 1000:\nPuns aren't jokes.|...they're punchlines. 1250:\nThe bartender says. \"How much is $0.5 for an idiot?\" The mathematician says \"Not that much, I just went bowling. 1500:\nWhy did the chicken cross the road?|Because it had no legs. What do you get when you cross a sheep with a goat?|I'm a sheep with no legs. Just look at that sheep! I've been in some really terrible relationships and wanted to share.|So I made some tea and got out.` After Training When you’ve finished training, you’ll need to copy the checkpointed model weights to the model directory like so:\ncp -r /gpt-2/checkpoint/run1/* /gpt-2/models/117M/\nNow, you can use your fine-tuned joke model to generate a bunch of jokes for a certain prompt. Let’s figure out more ways the chicken could cross the road…\npython src/interactive_conditional_samples.py --top_k 40 --temperature 0.7 --nsamples 10 [... wait for startup...] Model prompt \u003e\u003e\u003e Why did the chicken cross the road?| Some answers:\n\u003e So I can get my steak \u003e To the left... I'll see myself out \u003e Because I thought the chicken crossed the road. \u003e I don't know, but his face is too close to the road. \u003e Because it was a chicken. \u003e The chicken crossed the road in the morning, and the morning is fine, the morning is fine. What happens when A guy walks into a bar...?\nModel prompt \u003e\u003e\u003e A guy walks into a bar...| \u003e A guy walks into a bar and orders a drink. The bartender asks him what the occasion is. The guy says that it's the new year and all the old ladies are going to get married. The bartender says no worries, they'll be there in time for the anniversary. The guy says that he's really excited and he's a bit nervous. The bartender is a bit more hesitant and asks him what the occasion is. The guy says \"I'm a new guy, I was going to give a speech about my experience with alcohol.\" So the bartender gives him a little push and the guy says \"You know what, I'm drunk.\" Then the bartender says \"Sorry, but you don't have the guts to talk like that.\" What about What's the difference between?\nModel prompt \u003e\u003e\u003e What's the difference between \" a woman that's going to have a baby and a guy that's never gonna have a baby?|One's a girl that's going to have a baby and the other is a guy that's never gonna have a baby. Hilarious.\nWrap-up My personal thought is that we’ve probably overfit on the data. The examples I’m not selecting are pretty reflective of the typical jokes on /r/jokes (lots of dogs, “whats the difference?” and racial stereotyping). For my sense of humor, favoring a little more generality and abstractness, I’d prefer text from the models after 500-1000 epochs of training.\n","wordCount":"1432","inLanguage":"en","datePublished":"2019-03-23T11:16:39-04:00","dateModified":"2019-03-23T11:16:39-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/gpt2-jokes/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.peterbaumgartner.com/ title=Home><span>Home</span></a></li><li><a href=https://www.peterbaumgartner.com/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks><span>Notebooks</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Fine-Tuning GPT-2 Small for Generative Text</h1><div class=post-meta><span title='2019-03-23 11:16:39 -0400 -0400'>March 23, 2019</span></div></header><div class=post-content><meta name=twitter:card content="summary"><meta name=twitter:site content="@pmbaumgartner"><meta name=twitter:creator content="@pmbaumgartner"><meta name=twitter:title content="Fine-Tuning GPT-2 Small for Generative Text"><meta name=twitter:description content="Finally, you too can force a robot to tell you why the chicken crossed the road."><meta name=twitter:image content="https://i.postimg.cc/qBNcrC6V/Screen-Shot-2019-03-23-at-2-17-43-PM.png"><blockquote><p>Why did the chicken cross the road? Because it had no legs.</p></blockquote><p>These are the types of <em>hilarious</em> jokes the <code>gpt-2 small</code> model can generate for you.</p><p>After reading a few blog posts <a href=https://www.gwern.net/RNN-metadata#finetuning-the-gpt-2-small-transformer-for-english-poetry-generation>here</a> and <a href=https://svilentodorov.xyz/blog/gpt-finetune>here</a>, and playing around with <code>gpt-2 small</code> myself, I thought I would write up the full process I used to fine-tune and produce generative text.</p><p>For this example, we&rsquo;ll use a dataset of jokes pulled from the <code>/r/jokes</code> subreddit to fine tune the GPT-2 small model to generate new jokes. You&rsquo;ll need a computer with a GPU and <code>nvidia-docker</code> for this.</p><h2 id=getting-started>Getting Started<a hidden class=anchor aria-hidden=true href=#getting-started>#</a></h2><p>We&rsquo;ll start by cloning the code to download and train the GPT-2 Small model. Fortunately, others have done the hard work of adding code to train on top of the <code>gpt-2 small</code> model that OpenAI released.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/nshepperd/gpt-2
</span></span><span style=display:flex><span>cd gpt-2
</span></span><span style=display:flex><span>sh download_model.sh 117M
</span></span></code></pre></div><p>We&rsquo;re going to use docker from here on out, just because it&rsquo;s easier to manage the code and dependencies. The repository comes with a dockerfile, let&rsquo;s build the image:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker build --tag gpt-2 -f Dockerfile.gpu .
</span></span></code></pre></div><p>Great! Let&rsquo;s get to a shell using our image:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --runtime<span style=color:#f92672>=</span>nvidia -it <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -v <span style=color:#66d9ef>$(</span>pwd<span style=color:#66d9ef>)</span>:/gpt-2 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -e NVIDIA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  gpt-2 bash
</span></span></code></pre></div><p>At this point, you can play with the base <code>gpt-2 small</code> model and generate some text. Let&rsquo;s try it out with this prompt:</p><blockquote><p>&ldquo;A pair of jumper cables walks into a bar&rdquo;</p></blockquote><pre tabindex=0><code>$ python src/interactive_conditional_samples.py --top_k 40 --temperature 0.9
[...some tensorflow logging]
Model prompt &gt;&gt;&gt; A pair of jumper cables walks into a bar
======================================== SAMPLE 1 ========================================
 and a few minutes later, they are all turned on. A man in jeans and a black dress, comes out of the bar and says he got a job and wanted to know if he could tell me what he is wearing, how much he is wearing and whether he&#39;s wearing something different. He&#39;s wearing a red skirt, a blue T-shirt and black heels, on a black tie, like a red cape with two red crosses, the same as on the white one.
</code></pre><p>The original punchline was:</p><blockquote><p>The bartender sighs and says; &ldquo;I&rsquo;ll serve you, but don&rsquo;t start anything!&rdquo;</p></blockquote><p>That&rsquo;s the level of sense we can expect out of this thing without fine tuning.</p><h2 id=fine-tuning-on-a-specific-corpus>Fine-Tuning on a Specific Corpus<a hidden class=anchor aria-hidden=true href=#fine-tuning-on-a-specific-corpus>#</a></h2><p>We&rsquo;re going to fine-tune it a set of jokes. What this is going to do is train the model to pick up both the structure of the joke (setup, punchline) as well as how language is used (both vocabulary and structure).</p><p>We&rsquo;ll download a set of jokes from <a href=https://github.com/taivop/joke-dataset>this repository</a>. Note that there is some NSFW and racist, sexist, and plain unfunny content in some of these jokes, look out for this both in the training data and in our model output.</p><p>There are a few different structures for jokes in our dataset. For short jokes, the setup will be the title of the post, and the punchline will be the body. For longer jokes that have a bit of setup, typically the title of the post is also the first sentence of the joke. Fine-tuning the model will pick up on the structure and language of the jokes, so what we&rsquo;ll do is separate the setup (post title) and punchline (post body) with a pipe (<code>|</code>). Additionally, <code>gpt-2</code> uses a special token <code>&lt;|endoftext|></code>, to signify the end of a piece of text, so we&rsquo;ll be formatting data with those structural constraints.</p><p>First, let&rsquo;s download the data:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -O https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json
</span></span></code></pre></div><p>The following python script will get the data in the format we need:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pathlib <span style=color:#f92672>import</span> Path
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>jokes_raw <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(Path(<span style=color:#e6db74>&#34;reddit_jokes.json&#34;</span>)<span style=color:#f92672>.</span>read_text())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>jokes_parsed <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&lt;|endoftext|&gt;&#34;</span><span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{0}</span><span style=color:#e6db74>|</span><span style=color:#e6db74>{1}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(j[<span style=color:#e6db74>&#39;title&#39;</span>], j[<span style=color:#e6db74>&#39;body&#39;</span>]) <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> jokes_raw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Path(<span style=color:#e6db74>&#34;input-text.txt&#34;</span>)<span style=color:#f92672>.</span>write_text(jokes_parsed)
</span></span></code></pre></div><p>(side note: <code>pathlib</code> is amazing, you should be using it if you aren&rsquo;t already)</p><p>Now we must encode our text in the input format for gpt-2. This is something that normally would happen during training anyway, but we can speed it up by preprocessing.</p><p>The <code>requirements.txt</code> needs to be updated with the <code>tqdm</code> dependency, so you will need to install it in the container before running the encoding script.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install tqdm
</span></span><span style=display:flex><span>PYTHONPATH<span style=color:#f92672>=</span>src ./encode.py input-text.txt input-text.txt.npz
</span></span></code></pre></div><p>And with that, let&rsquo;s get training! This script will train until you <code>Ctrl-C</code> out of it. The flags that we&rsquo;re passing will print out a sample as well as checkpoint the model every 250 epochs.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>PYTHONPATH<span style=color:#f92672>=</span>src ./train.py --dataset input-text.txt.npz --sample_every<span style=color:#f92672>=</span><span style=color:#ae81ff>250</span> --save_every<span style=color:#f92672>=</span><span style=color:#ae81ff>250</span>
</span></span></code></pre></div><p>Choosing the right amount of training for these types of things is difficult. Personally, I enjoy the surrealism, absurdity, and nonsense of jokes from models with fewer training epochs.</p><h3 id=training-examples>Training Examples<a hidden class=anchor aria-hidden=true href=#training-examples>#</a></h3><p>Below I&rsquo;ll post some jokes from the sample output that gets generated every 250 epochs:</p><p>250:</p><pre tabindex=0><code>What do you call it when the baby is on your chin?|Lion&#39;s Pie
Did you hear about the guy who lost a job for eating his lunch?|He was just going to eat it.
I tried to take my dog to the bathroom in the hospital...|My dog was a dachshund.
What did the priest say after an elephant died?|&#34;What is it for?&#34;
</code></pre><p>500 epochs:</p><pre tabindex=0><code>I bought my car last week, and it wasn&#39;t working really well.|I was going to give it a better start, but the engine just wasn&#39;t good enough.
</code></pre><p>750:</p><pre tabindex=0><code>Two dogs are walking through the woods|When they meet up the first dog yells &#34;Hey is this dog our friend?&#34;. The second dog thinks it&#39;s funny and says &#34;Is this dog that he walks around alone&#34;.

And the two dogs quickly stop walking.

&#34;I don&#39;t think so, your mom always walks by his yard.&#34;
Why did the chicken cross the road?|To get a job at the hen.
What do you call a man with dyslexia?|A dyslexic.
A man walks into a bar and asks for a drink|Bartender looks at him and says, &#34;This is not a drinking game.&#34;
</code></pre><p>1000:</p><pre tabindex=0><code>Puns aren&#39;t jokes.|...they&#39;re punchlines.
</code></pre><p>1250:</p><pre tabindex=0><code>The bartender says.

&#34;How much is $0.5 for an idiot?&#34; The mathematician says &#34;Not that much, I just went bowling.
</code></pre><p>1500:</p><pre tabindex=0><code>Why did the chicken cross the road?|Because it had no legs.
What do you get when you cross a sheep with a goat?|I&#39;m a sheep with no legs. Just look at that sheep!
I&#39;ve been in some really terrible relationships and wanted to share.|So I made some tea and got out.`
</code></pre><h3 id=after-training>After Training<a hidden class=anchor aria-hidden=true href=#after-training>#</a></h3><p>When you&rsquo;ve finished training, you&rsquo;ll need to copy the checkpointed model weights to the model directory like so:</p><p><code>cp -r /gpt-2/checkpoint/run1/* /gpt-2/models/117M/</code></p><p>Now, you can use your fine-tuned joke model to generate a bunch of jokes for a certain prompt. Let&rsquo;s figure out more ways the chicken could cross the road&mldr;</p><pre tabindex=0><code>python src/interactive_conditional_samples.py --top_k 40 --temperature 0.7 --nsamples 10
[... wait for startup...]
Model prompt &gt;&gt;&gt; Why did the chicken cross the road?|
</code></pre><p>Some answers:</p><pre tabindex=0><code>&gt; So I can get my steak
&gt; To the left... I&#39;ll see myself out
&gt; Because I thought the chicken crossed the road.
&gt; I don&#39;t know, but his face is too close to the road.
&gt; Because it was a chicken.
&gt; The chicken crossed the road in the morning, and the morning is fine, the morning is fine.
</code></pre><p>What happens when <code>A guy walks into a bar...</code>?</p><pre tabindex=0><code>Model prompt &gt;&gt;&gt; A guy walks into a bar...|
&gt; A guy walks into a bar and orders a drink. The bartender asks him what the occasion is. The guy says that it&#39;s the new year and all the old ladies are going to get married. The bartender says no worries, they&#39;ll be there in time for the anniversary. The guy says that he&#39;s really excited and he&#39;s a bit nervous. The bartender is a bit more hesitant and asks him what the occasion is. The guy says &#34;I&#39;m a new guy, I was going to give a speech about my experience with alcohol.&#34; So the bartender gives him a little push and the guy says &#34;You know what, I&#39;m drunk.&#34; Then the bartender says &#34;Sorry, but you don&#39;t have the guts to talk like that.&#34;
</code></pre><p>What about <code>What's the difference between</code>?</p><pre tabindex=0><code>Model prompt &gt;&gt;&gt; What&#39;s the difference between
&#34; a woman that&#39;s going to have a baby and a guy that&#39;s never gonna have a baby?|One&#39;s a girl that&#39;s going to have a baby and the other is a guy that&#39;s never gonna have a baby.
</code></pre><p>Hilarious.</p><h2 id=wrap-up>Wrap-up<a hidden class=anchor aria-hidden=true href=#wrap-up>#</a></h2><p>My personal thought is that we&rsquo;ve probably overfit on the data. The examples I&rsquo;m not selecting are pretty reflective of the typical jokes on <code>/r/jokes</code> (lots of dogs, &ldquo;whats the difference?&rdquo; and racial stereotyping). For my sense of humor, favoring a little more generality and abstractness, I&rsquo;d prefer text from the models after 500-1000 epochs of training.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body><script data-goatcounter=https://peterbaumgartner.goatcounter.com/count async src=//gc.zgo.at/count.js></script></html>