<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on NLP Projects | Peter Baumgartner</title><meta name=keywords content><meta name=description content="Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal&rsquo;s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.
In NLP and ML we talk a lot about models and optimization. But this isn't where the battle is really won! I've been trying to explain my thoughts on this lately."><meta name=author content><link rel=canonical href=https://www.peterbaumgartner.com/blog/notes-on-nlp-projects/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.peterbaumgartner.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.peterbaumgartner.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.peterbaumgartner.com/favicon-32x32.png><link rel=apple-touch-icon href=https://www.peterbaumgartner.com/apple-touch-icon.png><link rel=mask-icon href=https://www.peterbaumgartner.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.109.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-72692144-1","auto"),ga("send","pageview"))</script><meta property="og:title" content="Notes on NLP Projects"><meta property="og:description" content="Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal&rsquo;s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.
In NLP and ML we talk a lot about models and optimization. But this isn't where the battle is really won! I've been trying to explain my thoughts on this lately."><meta property="og:type" content="article"><meta property="og:url" content="https://www.peterbaumgartner.com/blog/notes-on-nlp-projects/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-08-06T10:59:48-04:00"><meta property="article:modified_time" content="2018-08-06T10:59:48-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Notes on NLP Projects"><meta name=twitter:description content="Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal&rsquo;s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.
In NLP and ML we talk a lot about models and optimization. But this isn't where the battle is really won! I've been trying to explain my thoughts on this lately."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://www.peterbaumgartner.com/blog/"},{"@type":"ListItem","position":2,"name":"Notes on NLP Projects","item":"https://www.peterbaumgartner.com/blog/notes-on-nlp-projects/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on NLP Projects","name":"Notes on NLP Projects","description":"Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal\u0026rsquo;s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.\nIn NLP and ML we talk a lot about models and optimization. But this isn\u0026#39;t where the battle is really won! I\u0026#39;ve been trying to explain my thoughts on this lately.","keywords":[],"articleBody":" Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibalâ€™s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.\nIn NLP and ML we talk a lot about models and optimization. But this isn't where the battle is really won! I've been trying to explain my thoughts on this lately. Big thanks to @PyDataBerlin for a great event.\nðŸ“º Slides: https://t.co/OqpBeY0uwPhttps://t.co/TzhQrBsefP\nâ€” Matthew Honnibal (@honnibal) August 2, 2018 Essential Questions from the Machine Learning Hierarchy of Needs How will the model fit into the larger application or business process? What is the decision that will be impacted by a machine learning solution? Is there an existing software application we can plug into? Can we develop an API? Do we need to build something entirely custom? What is the annotation scheme? What is being labeled? How will the corpus be built? Are we classifying documents? Are we tagging spans of text? Are we trying to automatically generate structured data from text? What models do we need to build and compose to generate this data? What quality control processes will we use to ensure consistent and clean data? Who are experts who we can get to label data accurately? How will we construct a gold standard data set for evaluation? How can we break down the NLP problem to improve the ease and consistency of the annotation task? Things to Avoid Overambitious Imagineering â€” Thinking too big, rather than specifically about what business problem(s) can be solved. Forecasting without experimentation â€” Most problems will need to be evaluated after experimentation with data: donâ€™t put excessive value on arbitrary thresholds of accuracy or other performance metrics up front Outsourcing â€” You want a dataset and annotation process that is reliable and consistent, not noisy. Outsourcing solves the wrong problem (marginal annotation cost). Overengineering \u0026 AI FOMO â€” Can you compose existing, generic models to solve your task rather than tweaking parameters on the latest algorithms? Premature Shipping â€” if it fails in development, it certainly wonâ€™t work in production. General Principles (with timestamp) Iterate on your data (labeling, annotation scheme, problem framing), as well as your code. (10:52) Rather than make assumptions, get to iterative experimentation as fast as possible. (11:14) Start with what you need at output, then work towards breaking down the problem into a series of modeling decisions. (12:17) Break down the problem so that your component models can perform accurately and you can cheaply generate annotations. (14:00) Compose generic models into novel solutions (17:00) Example: Is this document about a crime? If so, what do the entities represent? By comparison: jumping straight to classifying the victim, perpetrator, location is both hard for a model to classify and cognitively burdensome for a human to annotate. Fine-tuning existing models for your domain, using generic categories, is much cheaper than starting from scratch (17:20) Example: fine tuning the dependency parsing on conversational or colloquial text might improve the accuracy. (33:50) Annotate events and topics at the sentence, paragraph, or document level to avoid boundary issues. Additionally, annotation is easier and classification is more accurate. (17:50) Bootstrap your project estimates based on contact with the evidence, rather than making assumptions or guesses up front. (23:17) Reframe your problem so that the annotator has to consider less information at once: use pre-suggested outputs and make corrections. This reduces error by asking the annotator to correct the model when its wrong (vs. getting tired/lazy on annotation tasks from scratch) (28:57) References:\nHonnibal, M. (2018, July). Building new NLP solutions with spaCy and Prodigy. Presented at PyData Berlin 2018, Berlin, Germany.\n","wordCount":"617","inLanguage":"en","datePublished":"2018-08-06T10:59:48-04:00","dateModified":"2018-08-06T10:59:48-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.peterbaumgartner.com/blog/notes-on-nlp-projects/"},"publisher":{"@type":"Organization","name":"Peter Baumgartner","logo":{"@type":"ImageObject","url":"https://www.peterbaumgartner.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://www.peterbaumgartner.com/ accesskey=h title="Peter Baumgartner (Alt + H)">Peter Baumgartner</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.peterbaumgartner.com/ title=Home><span>Home</span></a></li><li><a href=https://www.peterbaumgartner.com/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://www.peterbaumgartner.com/notebooks/ title=Notebooks><span>Notebooks</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Notes on NLP Projects</h1><div class=post-meta><span title='2018-08-06 10:59:48 -0400 -0400'>August 6, 2018</span></div></header><div class=post-content><meta name=twitter:card content="summary"><meta name=twitter:site content="@pmbaumgartner"><meta name=twitter:creator content="@pmbaumgartner"><meta name=twitter:title content="Notes on NLP Projects"><meta name=twitter:description content="These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal's PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy."><meta name=twitter:image content="https://s8.postimg.cc/t1tf3rg2d/Screen_Shot_2018-08-06_at_11.51.25_AM.png"><p><strong>Summary</strong>: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal&rsquo;s PyData Berlin 2018 talk: <em>Building new NLP solutions with spaCy and Prodigy</em>. I intended to use these as a reference when starting new NLP projects.</p><blockquote class=twitter-tweet data-lang=en><p lang=en dir=ltr>In NLP and ML we talk a lot about models and optimization. But this isn't where the battle is really won! I've been trying to explain my thoughts on this lately. Big thanks to <a href="https://twitter.com/pydataberlin?ref_src=twsrc%5Etfw">@PyDataBerlin</a> for a great event.<br><br>ðŸ“º Slides: <a href=https://t.co/OqpBeY0uwP>https://t.co/OqpBeY0uwP</a><a href=https://t.co/TzhQrBsefP>https://t.co/TzhQrBsefP</a></p>&mdash; Matthew Honnibal (@honnibal) <a href="https://twitter.com/honnibal/status/1025093783426359296?ref_src=twsrc%5Etfw">August 2, 2018</a></blockquote><script async src=https://platform.twitter.com/widgets.js></script><h3 id=essential-questions-from-the-machine-learning-hierarchy-of-needs>Essential Questions from the Machine Learning Hierarchy of Needs<a hidden class=anchor aria-hidden=true href=#essential-questions-from-the-machine-learning-hierarchy-of-needs>#</a></h3><ol><li><strong>How will the model fit into the larger application or business process?</strong><ol><li>What is the <em>decision</em> that will be impacted by a machine learning solution?</li><li>Is there an existing software application we can plug into? Can we develop an API?</li><li>Do we need to build something entirely custom?</li></ol></li><li><strong>What is the annotation scheme? What is being labeled? How will the corpus be built?</strong><ol><li>Are we classifying documents?</li><li>Are we tagging spans of text?</li><li>Are we trying to automatically generate structured data from text?<ol><li>What models do we need to build and compose to generate this data?</li></ol></li></ol></li><li><strong>What quality control processes will we use to ensure consistent and clean data?</strong><ol><li>Who are experts who we can get to label data accurately?</li><li>How will we construct a gold standard data set for evaluation?</li><li>How can we break down the NLP problem to improve the ease and consistency of the annotation task?</li></ol></li></ol><h3 id=things-to-avoid>Things to Avoid<a hidden class=anchor aria-hidden=true href=#things-to-avoid>#</a></h3><ol><li><strong>Overambitious <em>Imagineering</em></strong> â€” Thinking too big, rather than specifically about what business problem(s) can be solved.</li><li><strong>Forecasting without experimentation</strong> â€” Most problems will need to be evaluated after experimentation with data: don&rsquo;t put excessive value on arbitrary thresholds of accuracy or other performance metrics up front</li><li><strong>Outsourcing</strong> â€” You want a dataset and annotation process that is reliable and consistent, not noisy. Outsourcing solves the wrong problem (marginal annotation cost).</li><li><strong>Overengineering & AI FOMO</strong> â€” Can you compose existing, generic models to solve your task rather than tweaking parameters on the latest algorithms?</li><li><strong>Premature Shipping</strong> â€” if it fails in development, it certainly won&rsquo;t work in production.</li></ol><h3 id=general-principles-with-timestamp>General Principles (with timestamp)<a hidden class=anchor aria-hidden=true href=#general-principles-with-timestamp>#</a></h3><ul><li>Iterate on your data (labeling, annotation scheme, problem framing), as well as your code. (10:52)</li><li>Rather than make assumptions, get to iterative experimentation as fast as possible. (11:14)</li><li>Start with what you need at output, then work towards breaking down the problem into a series of modeling decisions. (12:17)</li><li>Break down the problem so that your component models can perform accurately and you can cheaply generate annotations. (14:00) Compose generic models into novel solutions (17:00)<ul><li><em>Example</em>: Is this document about a crime? If so, what do the entities represent? <em>By comparison</em>: jumping straight to classifying the victim, perpetrator, location is both hard for a model to classify and cognitively burdensome for a human to annotate.</li></ul></li><li>Fine-tuning existing models for your domain, using generic categories, is much cheaper than starting from scratch (17:20)<ul><li><em>Example</em>: fine tuning the dependency parsing on conversational or colloquial text might improve the accuracy. (33:50)</li></ul></li><li>Annotate events and topics at the sentence, paragraph, or document level to avoid boundary issues. Additionally, annotation is easier and classification is more accurate. (17:50)</li><li>Bootstrap your project estimates based on contact with the evidence, rather than making assumptions or guesses up front. (23:17)</li><li>Reframe your problem so that the annotator has to consider less information at once: use pre-suggested outputs and make corrections. This reduces error by asking the annotator to correct the model when its wrong (vs. getting tired/lazy on annotation tasks from scratch) (28:57)</li></ul><p><strong>References:</strong></p><p>Honnibal, M. (2018, July). <em>Building new NLP solutions with spaCy and Prodigy</em>. Presented at PyData Berlin 2018, Berlin, Germany.</p></div><footer class=post-footer></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://www.peterbaumgartner.com/>Peter Baumgartner</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body><script data-goatcounter=https://peterbaumgartner.goatcounter.com/count async src=//gc.zgo.at/count.js></script></html>